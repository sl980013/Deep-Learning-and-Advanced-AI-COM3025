{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Library"
      ],
      "metadata": {
        "id": "Cv-9Vzunb_tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import os\n",
        "import urllib.request\n",
        "from functools import partial\n",
        "from urllib.error import HTTPError"
      ],
      "metadata": {
        "id": "4f-K54nHb-Uq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Attention?\n",
        "\n",
        "Attention in neural networks, particularly relevant for sequential tasks, refers to a mechanism that selectively focuses on certain parts of input data. This concept has gained significant interest in recent years. In essence, attention computes a weighted average of elements in a sequence, with the weights being dynamically determined based on the relevance of each element to a specific query. This allows the model to prioritize certain inputs over others.\n",
        "\n",
        "The attention mechanism consists of four primary components:\n",
        "\n",
        "* **Query**: A feature vector representing the target of the attention, essentially indicating the information the model seeks within the sequence.\n",
        "* **Keys**: Feature vectors corresponding to each input element, describing the content or relevance of the elements. The keys help the model identify which elements to focus on, relative to the query.\n",
        "* **Values**: Feature vectors representing the actual content from each input element that the model should aggregate.\n",
        "* **Score function**: A function used to calculate attention weights, representing the relevance of each key-query pair. Common implementations include simple operations like the dot product or more complex structures like a small neural network.\n",
        "\n",
        "The attention mechanism operates by first computing scores between the query and each key using the score function. These scores determine the attention weights through a softmax function, ensuring that they sum to one and are non-negative. The output is then calculated as the weighted sum of the value vectors, with weights corresponding to the calculated attention scores.\n",
        "\n",
        "Mathematically, this process can be represented as:\n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
        "$$\n",
        "\n",
        "In practice, attention mechanisms can vary based on the choice of queries, the definition of key and value vectors, and the specific score function used. A prominent example is the **self-attention** mechanism used in the Transformer architecture, where each element in a sequence provides its own key, value, and query. The self-attention mechanism allows each element to attend to all elements in the sequence, including itself, resulting in a representation that incorporates information from the entire sequence.\n",
        "\n",
        "The above explanation provides a conceptual understanding of the attention mechanism, highlighting its components and operational principles without delving into the specific details of any particular implementation, such as the scaled dot product attention used in Transformers."
      ],
      "metadata": {
        "id": "BtW5eDFocsMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled Dot Product Attention\n",
        "\n",
        "The scaled dot product attention is a fundamental component of the self-attention mechanism, enabling elements within a sequence to efficiently attend to one another. It operates on queries $Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$, and values $V\\in\\mathbb{R}^{T\\times d_v}$, where $T$ represents the sequence length and $d_k$, $d_v$ denote the dimensions of queries/keys and values, respectively.\n",
        "\n",
        "The mechanism calculates the attention values based on the dot product similarity between each query $Q_i$ and key $K_j$, and scales the results by the square root of the dimensionality of the keys, $d_k$. The formula for this calculation is:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Here, the matrix product $QK^T$ computes the dot product between all pairs of queries and keys, forming a $T\\times T$ matrix where each entry represents the attention score from one element to another. After applying the softmax function, these scores are used as weights to compute a weighted average of the value vectors.\n",
        "\n",
        "The scaling factor $1/\\sqrt{d_k}$ is critical for maintaining the variance of the attention scores at an appropriate level. Without this scaling, the variance of the dot products could become too large, leading to a situation where the softmax function saturates, with most of its output concentrated on a single element. This would hinder learning by resulting in gradients that are almost zero.\n",
        "\n",
        "Additionally, the mechanism can include an optional masking step (denoted as `Mask (opt.)` in the diagram), useful in situations like batch processing of sequences of varying lengths. Padding is used to equalize the lengths of sequences, and the mask ensures that the padded positions do not affect the attention calculation, typically by assigning a very low value to these positions in the attention scores.\n",
        "\n",
        "In summary, the scaled dot product attention efficiently enables each element in a sequence to attend to all others, considering the relevance of each element, and is crucial for models that rely on self-attention, such as Transformers."
      ],
      "metadata": {
        "id": "1DFh9Ic8dp-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Scaled Dot Product Attention\n",
        "\n",
        "Scaled dot product attention is a core mechanism allowing each element in a sequence to consider all other elements efficiently, which is fundamental in self-attention models like Transformers. Here's a detailed guide to implementing scaled dot product attention, breaking down the components and the steps involved.\n",
        "\n",
        "#### Inputs to the Attention Mechanism\n",
        "The attention function takes three inputs:\n",
        "1. **Queries (Q)**: $Q\\in\\mathbb{R}^{T\\times d_k}$, where $T$ is the sequence length and $d_k$ is the dimensionality of the queries and keys.\n",
        "2. **Keys (K)**: $K\\in\\mathbb{R}^{T\\times d_k}$.\n",
        "3. **Values (V)**: $V\\in\\mathbb{R}^{T\\times d_v}$, where $d_v$ is the dimensionality of the values.\n",
        "\n",
        "#### Step-by-Step Calculation\n",
        "1. **Dot Product of Queries and Keys**: Calculate the dot product between each query and all keys to obtain a measure of compatibility or relevance between each query-key pair. This results in a matrix of shape $T \\times T$, where each element $(i, j)$ represents the dot product between query $i$ and key $j$.\n",
        "   \n",
        "   $$\\text{Score Matrix} = QK^T$$\n",
        "\n",
        "2. **Scaling**: Scale the scores obtained in the previous step by dividing by $\\sqrt{d_k}$ to ensure stable gradients, as larger values of $d_k$ can lead to extremely small gradients, which can slow down learning and model convergence.\n",
        "\n",
        "   $$\\text{Scaled Score Matrix} = \\frac{\\text{Score Matrix}}{\\sqrt{d_k}}$$\n",
        "\n",
        "3. **Optional Masking**: If masking is required (e.g., for padded positions in a batch of sequences), apply the mask by setting the scores for masked positions to a very large negative value, ensuring that they have minimal impact after the softmax step.\n",
        "\n",
        "4. **Softmax**: Apply the softmax function to the scaled scores along each row. This step converts the scores into probabilities, indicating the importance of each key relative to each query.\n",
        "\n",
        "   $$\\text{Attention Weights} = \\text{softmax}(\\text{Scaled Score Matrix})$$\n",
        "\n",
        "5. **Output Calculation**: Multiply the attention weights by the value vectors to obtain the final output. This step computes a weighted average of the value vectors, where the weights are determined by the attention scores.\n",
        "\n",
        "   $$\\text{Output} = \\text{Attention Weights} \\times V$$\n",
        "\n",
        "#### Implementation Tips\n",
        "- **Dimensionality**: Ensure the dimensions of your matrices are correct. Matrix multiplication will not be possible if the inner dimensions do not match.\n",
        "- **Numerical Stability**: When implementing the softmax function, ensure numerical stability by subtracting the maximum value in each row of the scores matrix before applying the exponential function.\n",
        "- **Batch Processing**: If implementing attention in batch, include an additional batch dimension in your matrices (e.g., $Q\\in\\mathbb{R}^{B\\times T\\times d_k}$ for a batch size of $B$) and ensure your implementation supports this.\n",
        "- **Testing**: Verify the correctness of your implementation with simple test cases to ensure it behaves as expected.\n",
        "\n",
        "This framework should provide a clear structure for students to implement scaled dot product attention, enhancing their understanding of its role and functionality in self-attention models."
      ],
      "metadata": {
        "id": "VKvaGxqIdvba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Please implement a scaled dot product function"
      ],
      "metadata": {
        "id": "jsFoInPLeFk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "  score_matrix = q @ k.transpose(-2, -1)\n",
        "  scaled_score = score_matrix / math.sqrt(k.shape[1])\n",
        "  attention_weight = torch.nn.functional.softmax(scaled_score, dim= 1)\n",
        "  output = attention_weight @ v\n",
        "  return output, attention_weight"
      ],
      "metadata": {
        "id": "XCv8_IzSdut4"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test case\n",
        "seq_len, d_k = 3, 2\n",
        "torch.manual_seed(3025)\n",
        "q = torch.randn(seq_len, d_k)\n",
        "k = torch.randn(seq_len, d_k)\n",
        "v = torch.randn(seq_len, d_k)\n",
        "valid = torch.tensor([[-1.0142, -1.9154],\n",
        "        [-0.4535, -1.6679],\n",
        "        [ 0.5474, -1.2476]])\n",
        "output, attention_weight = scaled_dot_product(q,k,v)\n",
        "differences = (output - valid).mean()\n",
        "print(q)\n",
        "print(k)\n",
        "print(v)\n",
        "print(output)\n",
        "print(differences)\n",
        "assert torch.abs(differences) < 0.0001, 'the product must be similar output as expected'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMIShH5wcrUK",
        "outputId": "b9aa19b0-1598-4284-e2de-b78dc8604968"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2840,  0.9623],\n",
            "        [ 1.0821, -0.2264],\n",
            "        [ 0.4840, -1.0348]])\n",
            "tensor([[ 0.0392,  0.2658],\n",
            "        [ 3.1410,  1.9842],\n",
            "        [ 1.2559, -1.1543]])\n",
            "tensor([[ 0.2172, -0.7752],\n",
            "        [-1.0788, -1.9513],\n",
            "        [ 0.9364, -1.2229]])\n",
            "tensor([[-1.0142, -1.9154],\n",
            "        [-0.4535, -1.6679],\n",
            "        [ 0.5474, -1.2476]])\n",
            "tensor(-1.2095e-05)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention\n",
        "\n",
        "Multi-Head Attention is an advancement over the scaled dot product attention, enabling the model to concurrently attend to information from different representation subspaces at different positions. This is particularly useful when dealing with complex data where different elements of the sequence may have different types of relevance or relationships to other elements.\n",
        "\n",
        "#### Concept\n",
        "Instead of a single attention \"head,\" Multi-Head Attention uses multiple sets of Query, Key, and Value weight matrices to project the input into different subspaces, allowing the model to capture various aspects of the information. Each set of projections is referred to as a \"head.\" The attention outputs from each head are then concatenated and linearly transformed into the expected dimension.\n",
        "\n",
        "#### Mathematical Representation\n",
        "Given Query, Key, and Value matrices (Q, K, V), the process can be mathematically described as:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
        "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "In this formula:\n",
        "- $W_i^Q \\in \\mathbb{R}^{D \\times d_k}$, $W_i^K \\in \\mathbb{R}^{D \\times d_k}$, and $W_i^V \\in \\mathbb{R}^{D \\times d_v}$ are parameter matrices for the $i$-th attention head.\n",
        "- $W^O \\in \\mathbb{R}^{h \\cdot d_k \\times d_{out}}$ is the parameter matrix for the linear transformation after concatenating the heads.\n",
        "- $D$ is the dimensionality of the input, $h$ is the number of heads, and $d_{out}$ is the output dimensionality.\n",
        "\n",
        "#### Integration in Neural Networks\n",
        "In a neural network, the Multi-Head Attention layer is typically applied to a feature map $X \\in \\mathbb{R}^{B \\times T \\times d_{\\text{model}}}$, where $B$ is the batch size, $T$ is the sequence length, and $d_{\\text{model}}$ is the dimensionality of the model's hidden layer. Here, $X$ serves as $Q$, $K$, and $V$. The transformation to query, key, and value representations is done using separate learnable weight matrices $W^Q$, $W^K$, and $W^V$.\n",
        "\n",
        "#### Implementation Notes\n",
        "- **Heads**: Each head captures different aspects of the input data. More heads allow the model to simultaneously focus on different subspaces.\n",
        "- **Dimensionality**: Ensure the dimensions of your weight matrices and inputs align correctly.\n",
        "- **Efficiency**: Despite the increased complexity, Multi-Head Attention can be efficiently parallelized, making it suitable for large-scale problems.\n",
        "\n",
        "By utilizing Multi-Head Attention, models can gain a more nuanced understanding of the data, capturing various types of relationships within the sequence. This is especially beneficial in complex tasks like language understanding, where different words or phrases may have different kinds of relationships with others in the sequence."
      ],
      "metadata": {
        "id": "DnDq4vT7kEGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)  # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "        values = values.permute(0, 2, 1, 3)  # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o"
      ],
      "metadata": {
        "id": "zOiDz_FkkDDm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder\n",
        "\n",
        "The Transformer Encoder plays a crucial role in transforming input sequences into rich, attention-based representations, primarily used in Sequence-to-Sequence tasks like machine translation. While the original Transformer model consists of both encoder and decoder, the encoder alone has been foundational in numerous advances in NLP and beyond. This section focuses on the encoder's architecture, function, and key components.\n",
        "\n",
        "#### Overview\n",
        "The Transformer Encoder is composed of a stack of $N$ identical layers, each containing two main sub-layers:\n",
        "\n",
        "1. **Multi-Head Attention Mechanism**: Enables the model to attend to different positions of the input sequence simultaneously.\n",
        "2. **Position-wise Feed-Forward Networks**: Consists of fully connected layers applied to each position separately, allowing for individual processing of each sequence element.\n",
        "\n",
        "#### Encoder Architecture\n",
        "Each layer in the encoder includes the following steps:\n",
        "\n",
        "1. **Input Processing**: The input $x$ (where $x$ can be $Q$, $K$, and $V$) is first passed through the Multi-Head Attention mechanism.\n",
        "2. **Residual Connection and Layer Normalization**: The output from the Multi-Head Attention is then added back to the input $x$ through a residual connection, followed by layer normalization:\n",
        "   \n",
        "   $$\\text{LayerNorm}(x + \\text{Multihead}(x, x, x))$$\n",
        "\n",
        "    The residual connections help in maintaining the flow of the original input information through the network and are crucial for training deeper models by improving gradient flow. Layer Normalization is used to stabilize the learning process and ensure consistent feature magnitude across sequence elements.\n",
        "\n",
        "3. **Position-wise Feed-Forward Networks (FFN)**: Each position is processed individually by a two-layered feed-forward network with ReLU activation in between:\n",
        "   \n",
        "   $$\n",
        "   \\begin{split}\n",
        "       \\text{FFN}(x) & = \\max(0, xW_1 + b_1)W_2 + b_2\\\\\n",
        "       x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
        "   \\end{split}\n",
        "   $$\n",
        "\n",
        "    This component allows for further processing of the information added by the attention mechanism, preparing it for the next layer.\n",
        "\n",
        "#### Considerations in Design\n",
        "- **Layer Normalization**: Chosen over Batch Normalization due to its independence from batch size and better performance in language tasks.\n",
        "- **Dimensionality of MLP in FFN**: Typically 2-8 times larger than the dimensionality of the input $x$ ($d_{\\text{model}}$), allowing for more complex transformations and faster parallelizable execution.\n",
        "- **Dropout**: Applied in MLP and on the outputs of MLP and Multi-Head Attention for regularization.\n",
        "\n",
        "The Transformer Encoder's architecture, with its repetitive yet intricate structure, allows for effective processing and transformation of sequence data, making it a powerful tool in various sequence modeling tasks. The next steps involve implementing the encoder block, paying close attention to the integration of Multi-Head Attention, residual connections, layer normalization, and feed-forward networks within each layer."
      ],
      "metadata": {
        "id": "sLI_NEVtlSNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        \"\"\"EncoderBlock.\n",
        "\n",
        "        Args:\n",
        "            input_dim: Dimensionality of the input\n",
        "            num_heads: Number of heads to use in the attention block\n",
        "            dim_feedforward: Dimensionality of the hidden layer in the MLP\n",
        "            dropout: Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "\n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim),\n",
        "        )\n",
        "\n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Attention part\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # MLP part\n",
        "        linear_out = self.linear_net(x)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        return x\n",
        "\n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        for layer in self.layers:\n",
        "            _, attn_map = layer.self_attn(x, mask=mask, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = layer(x)\n",
        "        return attention_maps\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"Positional Encoding.\n",
        "\n",
        "        Args:\n",
        "            d_model: Hidden dimensionality of the input.\n",
        "            max_len: Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
        "        self.register_buffer(\"pe\", pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "a1HiddBnlW4J"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence to Sequence Tasks\n",
        "\n",
        "Sequence to Sequence (Seq2Seq) tasks involve converting an input sequence into an output sequence, where the input and output may vary in length. This model structure is commonly used in applications like machine translation, text summarization, and more. Typically, a Seq2Seq model comprises an encoder to interpret the input sequence and a decoder to generate the output sequence autoregressively.\n",
        "\n",
        "#### Simplified Task: Sequence Reversal\n",
        "For educational purposes, we'll focus on a simplified Seq2Seq task: reversing a sequence of numbers. Despite its simplicity, this task is a good testbed for understanding Seq2Seq models, especially since it requires capturing long-term dependencies, something traditional RNNs might struggle with, but Transformers are well-equipped to handle.\n",
        "\n",
        "#### Task Description:\n",
        "- **Input**: A sequence of $N$ numbers ranging from $0$ to $M$.\n",
        "- **Output**: The reversed sequence of the input.\n",
        "\n",
        "In Numpy, if our input sequence is $x$, the desired output is $x$[::-1]. Although straightforward, this task provides a clear demonstration of a model's ability to handle sequences and understand dependencies across positions.\n",
        "\n",
        "#### Implementation Steps:\n",
        "- **Create a Dataset Class**: The first step is to create a dataset class that can generate sequences of numbers and their reversed counterparts. This class will be used to train and evaluate the Seq2Seq model.\n",
        "\n",
        "By starting with this simple task, we can focus on the mechanics and capabilities of the Transformer encoder in handling sequences, setting the stage for tackling more complex Seq2Seq tasks in the future."
      ],
      "metadata": {
        "id": "bKeMF9xLmQH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReverseDataset(data.Dataset):\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        return inp_data, labels\n",
        "\n",
        "dataset = partial(ReverseDataset, 10, 16)\n",
        "train_loader = data.DataLoader(dataset(10000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
        "val_loader = data.DataLoader(dataset(1000), batch_size=64, drop_last=True, shuffle=False)\n",
        "\n",
        "for inputs,labels in train_loader:\n",
        "  print(inputs.shape)"
      ],
      "metadata": {
        "id": "PSBkeOmtmPhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33318eb4-1035-4489-df70-fd0973503887"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n",
            "torch.Size([128, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compose the network"
      ],
      "metadata": {
        "id": "VZ52A-Hhma4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        model_dim,\n",
        "        num_classes,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        dropout=0.0,\n",
        "        input_dropout=0.0,\n",
        "    ):\n",
        "        \"\"\"TransformerPredictor.\n",
        "\n",
        "        Args:\n",
        "            input_dim: Hidden dimensionality of the input\n",
        "            model_dim: Hidden dimensionality to use inside the Transformer\n",
        "            num_classes: Number of classes to predict per sequence element\n",
        "            num_heads: Number of heads to use in the Multi-Head Attention blocks\n",
        "            num_layers: Number of encoder blocks to use.\n",
        "            dropout: Dropout to apply inside the model\n",
        "            input_dropout: Dropout to apply on the input features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Input dim -> Model dim\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Dropout(input_dropout), nn.Linear(input_dim, model_dim)\n",
        "        )\n",
        "        # Positional encoding for sequences\n",
        "        self.positional_encoding = PositionalEncoding(d_model=model_dim)\n",
        "        # Transformer\n",
        "        self.transformer = TransformerEncoder(\n",
        "            num_layers=num_layers,\n",
        "            input_dim=model_dim,\n",
        "            dim_feedforward=2 * model_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        # Output classifier per sequence lement\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(model_dim, model_dim),\n",
        "            nn.LayerNorm(model_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(model_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input features of shape [Batch, SeqLen, input_dim]\n",
        "            mask: Mask to apply on the attention outputs (optional)\n",
        "            add_positional_encoding: If True, we add the positional encoding to the input.\n",
        "                                      Might not be desired for some tasks.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        x = self.transformer(x, mask=mask)\n",
        "        x = self.output_net(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "JxlCGvdomaDJ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Writing Training Loop"
      ],
      "metadata": {
        "id": "uUuW7DbBnjsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# please create the model\n",
        "inp_dim = 10\n",
        "model_dim = 128\n",
        "num_classes = 10\n",
        "num_heads = 4\n",
        "num_layers = 1\n",
        "model = TransformerPredictor(inp_dim, model_dim, num_classes, num_heads, num_layers)\n",
        "# please create the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "# please train the model, with the whole training pipeline\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "train_loss = 0\n",
        "for i in range(0, 10):\n",
        "  train_loss = 0\n",
        "  for inputs, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    inp_data = F.one_hot(inputs, num_classes=10).float()\n",
        "    outputs = model(inp_data)\n",
        "    loss = criterion(outputs.view(-1,10), labels.view(-1))\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f\"Validation Loss: {train_loss / 10000}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZaOx-7qni7y",
        "outputId": "a1cac3b9-ed8e-4787-f72d-52b4361b3469"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.01665152714252472\n",
            "Validation Loss: 0.011367890775203705\n",
            "Validation Loss: 0.009333883261680604\n",
            "Validation Loss: 0.00773980547785759\n",
            "Validation Loss: 0.004923707568645477\n",
            "Validation Loss: 0.004334514749050141\n",
            "Validation Loss: 0.0011263601891696453\n",
            "Validation Loss: 1.5722232218831776e-05\n",
            "Validation Loss: 6.483765313168988e-06\n",
            "Validation Loss: 4.9829480471089484e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "Here is the evaluation code, can you do better than 2.0?"
      ],
      "metadata": {
        "id": "NVqbotkCrCSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validating the validation loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Validation loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_loss = 0\n",
        "    for inputs, labels in val_loader:\n",
        "      inp_data = F.one_hot(inputs, num_classes=10).float()\n",
        "      outputs = model(inp_data)\n",
        "      loss = criterion(outputs.view(1024,10), labels.view(-1))\n",
        "      val_loss += loss.item()\n",
        "    print(f\"Validation Loss: {val_loss / len(val_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkRNkGBspuZh",
        "outputId": "34856981-f2e1-4ca6-90f0-794c56ad7117"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0005773689326209326\n"
          ]
        }
      ]
    }
  ]
}