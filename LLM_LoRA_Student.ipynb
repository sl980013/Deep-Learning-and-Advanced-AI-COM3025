{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSqI2Yls-22s"
      },
      "source": [
        "# Introduction to Fine-Tuning Large Language Models with LoRA\n",
        "\n",
        "In this laboratory session, we will explore an advanced technique for fine-tuning large language models, specifically focusing on the application of Low-Rank Adaptation (LoRA). This method allows for efficient parameter updating and learning, making it feasible to adapt large-scale models like Facebook's OPT to new tasks without the need for extensive computational resources.\n",
        "\n",
        "#### Objectives:\n",
        "- Understand the principles behind LoRA and its advantages for fine-tuning large language models.\n",
        "- Learn how to apply LoRA to the OPT model, focusing on specific components (LoRA_A and LoRA_B) for adaptation.\n",
        "- Explore the use of Hugging Face's `transformers` and `datasets` libraries for model training and evaluation.\n",
        "- Gain practical experience in handling natural language processing tasks, such as question-answering, through the implementation and fine-tuning of the model.\n",
        "- Evaluate the performance of the fine-tuned model using standard NLP metrics such as BLEU, ROUGE, and Exact Match.\n",
        "\n",
        "#### Prerequisites:\n",
        "Before starting this lab, you should have a basic understanding of PyTorch, neural networks, and NLP concepts. Familiarity with Hugging Face libraries will also be beneficial.\n",
        "\n",
        "#### Lab Overview:\n",
        "The lab is structured as follows:\n",
        "1. **Environment Setup**: We will begin by setting up our working environment, including the installation of necessary libraries and loading of environmental variables.\n",
        "2. **Model Initialization and LoRA Configuration**: You will learn how to initialize the OPT model and configure it using LoRA by modifying specific layers while freezing the rest of the model to prevent unnecessary updates.\n",
        "3. **Data Preparation**: We will load and preprocess the data suitable for our task, creating custom datasets and dataloaders.\n",
        "4. **Fine-Tuning and Evaluation**: You will fine-tune the model on a question-answering task and evaluate its performance using several NLP metrics.\n",
        "5. **Results Analysis**: Finally, we will analyze the results and understand the impact of LoRA fine-tuning on the model's performance.\n",
        "\n",
        "By the end of this lab, you will have hands-on experience fine-tuning large language models using LoRA, which is a valuable skill in the field of artificial intelligence and natural language processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXXcQYsB_brZ"
      },
      "source": [
        "# Import basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:04:35.645103Z",
          "iopub.status.busy": "2024-03-10T12:04:35.644757Z",
          "iopub.status.idle": "2024-03-10T12:05:52.940065Z",
          "shell.execute_reply": "2024-03-10T12:05:52.938807Z",
          "shell.execute_reply.started": "2024-03-10T12:04:35.645076Z"
        },
        "id": "uTkHfH0EIR-A",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers[torch]\n",
        "# !pip install -q datasets\n",
        "# !pip install -q accelerate -U\n",
        "# !pip install -q py7zr\n",
        "# !pip install -q evaluate nltk rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:05:52.942474Z",
          "iopub.status.busy": "2024-03-10T12:05:52.942133Z",
          "iopub.status.idle": "2024-03-10T12:06:12.255551Z",
          "shell.execute_reply": "2024-03-10T12:06:12.254721Z",
          "shell.execute_reply.started": "2024-03-10T12:05:52.942443Z"
        },
        "id": "yaBv9HTJ_XFF",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import evaluate\n",
        "from torch import nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlCWhPoq__yZ"
      },
      "source": [
        "# Load Evaluator\n",
        "\n",
        "In this section of the code, we are initializing evaluation metrics for the language model we plan to fine-tune. Specifically, we use the `evaluate` library, which is a part of the Hugging Face ecosystem, designed for evaluating and comparing the performance of models across a wide range of NLP tasks.\n",
        "\n",
        "1. `bleu_scorer = evaluate.load('bleu')`: This line loads the BLEU (Bilingual Evaluation Understudy) scorer from the `evaluate` library. BLEU is a widely used metric for evaluating the quality of text which has been machine-translated from one natural language to another. It works by comparing the machine-generated text to one or more reference texts (typically human-generated) and computes a score indicating how similar they are, based on the presence of the same words and phrases. BLEU is particularly popular in tasks like machine translation but is also used in other contexts like text summarization.\n",
        "\n",
        "2. `rouge_scorer = evaluate.load('rouge')`: This line loads the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scorer. ROUGE is another popular evaluation metric used primarily in summarization tasks. Unlike BLEU, which is precision-oriented, ROUGE focuses on recall, meaning it measures how well the generated summaries cover the content present in the reference summaries. It compares the overlap of n-grams, word sequences, and word pairs between the computer-generated output and the reference texts.\n",
        "\n",
        "These metrics will be used later in the training process to evaluate how well the fine-tuned language model performs on specific NLP tasks, such as translation or summarization. Using these evaluation metrics allows us to quantitatively assess the quality of the generated text and make informed decisions about the model's performance and potential improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:12.257274Z",
          "iopub.status.busy": "2024-03-10T12:06:12.256689Z",
          "iopub.status.idle": "2024-03-10T12:06:15.142744Z",
          "shell.execute_reply": "2024-03-10T12:06:15.141698Z",
          "shell.execute_reply.started": "2024-03-10T12:06:12.257246Z"
        },
        "id": "DzY1tS5WAEaB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bleu_scorer = evaluate.load('bleu')\n",
        "rouge_scorer = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,621,440 || all params: 2,654,218,240 || trainable%: 0.09876505106075979\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
        "model_name = \"facebook/opt-2.7b\"\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from peft import AutoPeftModelForCausalLM\n",
        "# from transformers import AutoTokenizer\n",
        "# import torch\n",
        "\n",
        "# model = AutoPeftModelForCausalLM.from_pretrained(\"ybelkada/opt-350m-lora\").to(\"cuda\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-2.7b\")\n",
        "\n",
        "# model.eval()\n",
        "# inputs = tokenizer(\"Preheat the oven to 350 degrees and place the cookie dough\", return_tensors=\"pt\")\n",
        "\n",
        "# outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=50)\n",
        "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
        "\n",
        "# \"Preheat the oven to 350 degrees and place the cookie dough in the center of the oven. In a large bowl, combine the flour, baking powder, baking soda, salt, and cinnamon. In a separate bowl, combine the egg yolks, sugar, and vanilla.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSyMk-pHBFYy"
      },
      "source": [
        "# Adding some necessary code for OPT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:15.146782Z",
          "iopub.status.busy": "2024-03-10T12:06:15.146480Z",
          "iopub.status.idle": "2024-03-10T12:06:15.281553Z",
          "shell.execute_reply": "2024-03-10T12:06:15.280609Z",
          "shell.execute_reply.started": "2024-03-10T12:06:15.146754Z"
        },
        "id": "TdLgdCKYBE47",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # @title\n",
        "# # coding=utf-8\n",
        "# # Copyright 2022 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.\n",
        "# #\n",
        "# # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# # you may not use this file except in compliance with the License.\n",
        "# # You may obtain a copy of the License at\n",
        "# #\n",
        "# #     http://www.apache.org/licenses/LICENSE-2.0\n",
        "# #\n",
        "# # Unless required by applicable law or agreed to in writing, software\n",
        "# # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# # See the License for the specific language governing permissions and\n",
        "# # limitations under the License.\n",
        "# \"\"\" PyTorch OPT model.\"\"\"\n",
        "# import random\n",
        "# from typing import List, Optional, Tuple, Union\n",
        "\n",
        "# import torch\n",
        "# import torch.utils.checkpoint\n",
        "# from torch import nn\n",
        "# from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "# from transformers.activations import ACT2FN\n",
        "# from transformers.modeling_outputs import (\n",
        "#     BaseModelOutputWithPast,\n",
        "#     CausalLMOutputWithPast,\n",
        "#     QuestionAnsweringModelOutput,\n",
        "#     SequenceClassifierOutputWithPast,\n",
        "# )\n",
        "# from transformers.modeling_utils import PreTrainedModel\n",
        "# from transformers.utils import (\n",
        "#     add_code_sample_docstrings,\n",
        "#     add_start_docstrings,\n",
        "#     add_start_docstrings_to_model_forward,\n",
        "#     logging,\n",
        "#     replace_return_docstrings,\n",
        "# )\n",
        "# from transformers.models.opt.configuration_opt import OPTConfig\n",
        "\n",
        "# logger = logging.get_logger(__name__)\n",
        "\n",
        "# _CHECKPOINT_FOR_DOC = \"facebook/opt-350m\"\n",
        "# _CONFIG_FOR_DOC = \"OPTConfig\"\n",
        "\n",
        "# # Base model docstring\n",
        "# _EXPECTED_OUTPUT_SHAPE = [1, 8, 1024]\n",
        "\n",
        "# # SequenceClassification docstring\n",
        "# _CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"ArthurZ/opt-350m-dummy-sc\"\n",
        "# _SEQ_CLASS_EXPECTED_LOSS = 1.71\n",
        "# _SEQ_CLASS_EXPECTED_OUTPUT = \"'LABEL_0'\"\n",
        "\n",
        "# OPT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "#     \"facebook/opt-125m\",\n",
        "#     \"facebook/opt-350m\",\n",
        "#     \"facebook/opt-1.3b\",\n",
        "#     \"facebook/opt-2.7b\",\n",
        "#     \"facebook/opt-6.7b\",\n",
        "#     \"facebook/opt-13b\",\n",
        "#     \"facebook/opt-30b\",\n",
        "#     # See all OPT models at https://huggingface.co/models?filter=opt\n",
        "# ]\n",
        "\n",
        "\n",
        "# # Copied from transformers.models.bart.modeling_bart._make_causal_mask\n",
        "# def _make_causal_mask(\n",
        "#         input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Make causal mask used for bi-directional self-attention.\n",
        "#     \"\"\"\n",
        "#     bsz, tgt_len = input_ids_shape\n",
        "#     mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n",
        "#     mask_cond = torch.arange(mask.size(-1), device=device)\n",
        "#     mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
        "#     mask = mask.to(dtype)\n",
        "\n",
        "#     if past_key_values_length > 0:\n",
        "#         mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
        "#     return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
        "\n",
        "\n",
        "# def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
        "#     \"\"\"\n",
        "#     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
        "#     \"\"\"\n",
        "#     bsz, src_len = mask.size()\n",
        "#     tgt_len = tgt_len if tgt_len is not None else src_len\n",
        "\n",
        "#     expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
        "\n",
        "#     inverted_mask = 1.0 - expanded_mask\n",
        "\n",
        "#     return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
        "\n",
        "\n",
        "# class OPTLearnedPositionalEmbedding(nn.Embedding):\n",
        "#     \"\"\"\n",
        "#     This module learns positional embeddings up to a fixed maximum size.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "#         # OPT is set up so that if padding_idx is specified then offset the embedding ids by 2\n",
        "#         # and adjust num_embeddings appropriately. Other models don't have this hack\n",
        "#         self.offset = 2\n",
        "#         super().__init__(num_embeddings + self.offset, embedding_dim)\n",
        "\n",
        "#     def forward(self, attention_mask: torch.LongTensor, past_key_values_length: int = 0):\n",
        "#         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
        "#         attention_mask = attention_mask.long()\n",
        "\n",
        "#         # create positions depending on attention_mask\n",
        "#         positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n",
        "\n",
        "#         # cut positions if `past_key_values_length` is > 0\n",
        "#         positions = positions[:, past_key_values_length:]\n",
        "\n",
        "#         return super().forward(positions + self.offset)\n",
        "\n",
        "\n",
        "# class OPTDecoderLayer(nn.Module):\n",
        "#     def __init__(self, config: OPTConfig):\n",
        "#         super().__init__()\n",
        "#         self.embed_dim = config.hidden_size\n",
        "#         self.self_attn = OPTAttention(\n",
        "#             embed_dim=self.embed_dim,\n",
        "#             num_heads=config.num_attention_heads,\n",
        "#             dropout=config.attention_dropout,\n",
        "#             is_decoder=True,\n",
        "#             bias=config.enable_bias,\n",
        "#         )\n",
        "#         self.do_layer_norm_before = config.do_layer_norm_before\n",
        "#         self.dropout = config.dropout\n",
        "#         self.activation_fn = ACT2FN[config.activation_function]\n",
        "\n",
        "#         self.self_attn_layer_norm = nn.LayerNorm(\n",
        "#             self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine\n",
        "#         )\n",
        "#         self.fc1 = nn.Linear(self.embed_dim, config.ffn_dim, bias=config.enable_bias)\n",
        "#         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=config.enable_bias)\n",
        "#         self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n",
        "\n",
        "#     def forward(\n",
        "#             self,\n",
        "#             hidden_states: torch.Tensor,\n",
        "#             attention_mask: Optional[torch.Tensor] = None,\n",
        "#             layer_head_mask: Optional[torch.Tensor] = None,\n",
        "#             past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "#             output_attentions: Optional[bool] = False,\n",
        "#             use_cache: Optional[bool] = False,\n",
        "#     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
        "#             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
        "#                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "#             layer_head_mask (`torch.FloatTensor`, *optional*): mask for attention heads in a given layer of size\n",
        "#                 `(encoder_attention_heads,)`.\n",
        "#             output_attentions (`bool`, *optional*):\n",
        "#                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "#                 returned tensors for more detail.\n",
        "#             use_cache (`bool`, *optional*):\n",
        "#                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
        "#                 (see `past_key_values`).\n",
        "#             past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
        "#         \"\"\"\n",
        "\n",
        "#         residual = hidden_states\n",
        "\n",
        "#         # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n",
        "#         if self.do_layer_norm_before:\n",
        "#             hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "#         # Self Attention\n",
        "#         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "#             hidden_states=hidden_states,\n",
        "#             past_key_value=past_key_value,\n",
        "#             attention_mask=attention_mask,\n",
        "#             layer_head_mask=layer_head_mask,\n",
        "#             output_attentions=output_attentions,\n",
        "#         )\n",
        "#         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "#         hidden_states = residual + hidden_states\n",
        "\n",
        "#         # 350m applies layer norm AFTER attention\n",
        "#         if not self.do_layer_norm_before:\n",
        "#             hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "#         # Fully Connected\n",
        "#         hidden_states_shape = hidden_states.shape\n",
        "#         hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))\n",
        "#         residual = hidden_states\n",
        "\n",
        "#         # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n",
        "#         if self.do_layer_norm_before:\n",
        "#             hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "#         hidden_states = self.fc1(hidden_states)\n",
        "#         hidden_states = self.activation_fn(hidden_states)\n",
        "\n",
        "#         hidden_states = self.fc2(hidden_states)\n",
        "#         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "#         hidden_states = (residual + hidden_states).view(hidden_states_shape)\n",
        "\n",
        "#         # 350m applies layer norm AFTER attention\n",
        "#         if not self.do_layer_norm_before:\n",
        "#             hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "#         outputs = (hidden_states,)\n",
        "\n",
        "#         if output_attentions:\n",
        "#             outputs += (self_attn_weights,)\n",
        "\n",
        "#         if use_cache:\n",
        "#             outputs += (present_key_value,)\n",
        "\n",
        "#         return outputs\n",
        "\n",
        "\n",
        "# OPT_START_DOCSTRING = r\"\"\"\n",
        "#     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "#     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "#     etc.)\n",
        "\n",
        "#     This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "#     Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "#     and behavior.\n",
        "\n",
        "#     Parameters:\n",
        "#         config ([`OPTConfig`]):\n",
        "#             Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
        "#             load the weights associated with the model, only the configuration. Check out the\n",
        "#             [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# @add_start_docstrings(\n",
        "#     \"The bare OPT Model outputting raw hidden-states without any specific head on top.\",\n",
        "#     OPT_START_DOCSTRING,\n",
        "# )\n",
        "# class OPTPreTrainedModel(PreTrainedModel):\n",
        "#     config_class = OPTConfig\n",
        "#     base_model_prefix = \"model\"\n",
        "#     supports_gradient_checkpointing = True\n",
        "#     _no_split_modules = [\"OPTDecoderLayer\"]\n",
        "#     _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n",
        "\n",
        "#     def _init_weights(self, module):\n",
        "#         std = self.config.init_std\n",
        "#         if isinstance(module, nn.Linear):\n",
        "#             module.weight.data.normal_(mean=0.0, std=std)\n",
        "#             if module.bias is not None:\n",
        "#                 module.bias.data.zero_()\n",
        "#         elif isinstance(module, nn.Embedding):\n",
        "#             module.weight.data.normal_(mean=0.0, std=std)\n",
        "#             if module.padding_idx is not None:\n",
        "#                 module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "#     def _set_gradient_checkpointing(self, module, value=False):\n",
        "#         if isinstance(module, (OPTDecoder)):\n",
        "#             module.gradient_checkpointing = value\n",
        "\n",
        "\n",
        "# OPT_INPUTS_DOCSTRING = r\"\"\"\n",
        "#     Args:\n",
        "#         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "#             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
        "#             it.\n",
        "\n",
        "#             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "#             [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "#             [What are input IDs?](../glossary#input-ids)\n",
        "#         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "#             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "#             - 1 for tokens that are **not masked**,\n",
        "#             - 0 for tokens that are **masked**.\n",
        "\n",
        "#             [What are attention masks?](../glossary#attention-mask)\n",
        "\n",
        "#             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "#             [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "#             If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n",
        "#             `past_key_values`).\n",
        "\n",
        "#             If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n",
        "#             and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
        "#             information on the default strategy.\n",
        "#         head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
        "#             Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
        "\n",
        "#             - 1 indicates the head is **not masked**,\n",
        "#             - 0 indicates the head is **masked**.\n",
        "\n",
        "#         past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "#             Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
        "#             `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
        "#             `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "\n",
        "#             Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
        "#             blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "#             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "#             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "#             `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "#         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "#             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
        "#             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
        "#             model's internal embedding lookup matrix.\n",
        "#         use_cache (`bool`, *optional*):\n",
        "#             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "#             `past_key_values`).\n",
        "#         output_attentions (`bool`, *optional*):\n",
        "#             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "#             tensors for more detail.\n",
        "#         output_hidden_states (`bool`, *optional*):\n",
        "#             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "#             more detail.\n",
        "#         return_dict (`bool`, *optional*):\n",
        "#             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# class OPTDecoder(OPTPreTrainedModel):\n",
        "#     \"\"\"\n",
        "#     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`OPTDecoderLayer`]\n",
        "\n",
        "#     Args:\n",
        "#         config: OPTConfig\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, config: OPTConfig):\n",
        "#         super().__init__(config)\n",
        "#         self.dropout = config.dropout\n",
        "#         self.layerdrop = config.layerdrop\n",
        "#         self.padding_idx = config.pad_token_id\n",
        "#         self.max_target_positions = config.max_position_embeddings\n",
        "#         self.vocab_size = config.vocab_size\n",
        "\n",
        "#         self.embed_tokens = nn.Embedding(config.vocab_size, config.word_embed_proj_dim, self.padding_idx)\n",
        "#         self.embed_positions = OPTLearnedPositionalEmbedding(config.max_position_embeddings, config.hidden_size)\n",
        "\n",
        "#         if config.word_embed_proj_dim != config.hidden_size:\n",
        "#             self.project_out = nn.Linear(config.hidden_size, config.word_embed_proj_dim, bias=False)\n",
        "#         else:\n",
        "#             self.project_out = None\n",
        "\n",
        "#         if config.word_embed_proj_dim != config.hidden_size:\n",
        "#             self.project_in = nn.Linear(config.word_embed_proj_dim, config.hidden_size, bias=False)\n",
        "#         else:\n",
        "#             self.project_in = None\n",
        "\n",
        "#         # Note that the only purpose of `config._remove_final_layer_norm` is to keep backward compatibility\n",
        "#         # with checkpoints that have been fine-tuned before transformers v4.20.1\n",
        "#         # see https://github.com/facebookresearch/metaseq/pull/164\n",
        "#         if config.do_layer_norm_before and not config._remove_final_layer_norm:\n",
        "#             self.final_layer_norm = nn.LayerNorm(\n",
        "#                 config.hidden_size, elementwise_affine=config.layer_norm_elementwise_affine\n",
        "#             )\n",
        "#         else:\n",
        "#             self.final_layer_norm = None\n",
        "\n",
        "#         self.layers = nn.ModuleList([OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "#         self.gradient_checkpointing = False\n",
        "#         # Initialize weights and apply final processing\n",
        "#         self.post_init()\n",
        "\n",
        "#     def get_input_embeddings(self):\n",
        "#         return self.embed_tokens\n",
        "\n",
        "#     def set_input_embeddings(self, value):\n",
        "#         self.embed_tokens = value\n",
        "\n",
        "#     # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
        "#     def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
        "#         # create causal mask\n",
        "#         # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "#         combined_attention_mask = None\n",
        "#         if input_shape[-1] > 1:\n",
        "#             combined_attention_mask = _make_causal_mask(\n",
        "#                 input_shape,\n",
        "#                 inputs_embeds.dtype,\n",
        "#                 device=inputs_embeds.device,\n",
        "#                 past_key_values_length=past_key_values_length,\n",
        "#             )\n",
        "\n",
        "#         if attention_mask is not None:\n",
        "#             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "#             expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
        "#                 inputs_embeds.device\n",
        "#             )\n",
        "#             combined_attention_mask = (\n",
        "#                 expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
        "#             )\n",
        "\n",
        "#         return combined_attention_mask\n",
        "\n",
        "#     def forward(\n",
        "#             self,\n",
        "#             input_ids: torch.LongTensor = None,\n",
        "#             attention_mask: Optional[torch.Tensor] = None,\n",
        "#             head_mask: Optional[torch.Tensor] = None,\n",
        "#             past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "#             inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "#             use_cache: Optional[bool] = None,\n",
        "#             output_attentions: Optional[bool] = None,\n",
        "#             output_hidden_states: Optional[bool] = None,\n",
        "#             return_dict: Optional[bool] = None,\n",
        "#     ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
        "#         r\"\"\"\n",
        "#         Args:\n",
        "#             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "#                 Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "#                 provide it.\n",
        "\n",
        "#                 Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "#                 [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "#                 [What are input IDs?](../glossary#input-ids)\n",
        "#             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "#                 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "#                 - 1 for tokens that are **not masked**,\n",
        "#                 - 0 for tokens that are **masked**.\n",
        "\n",
        "#                 [What are attention masks?](../glossary#attention-mask)\n",
        "#             head_mask (`torch.Tensor` of shape `(num_hidden_layers, num_attention_heads)`, *optional*):\n",
        "#                 Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
        "\n",
        "#                 - 1 indicates the head is **not masked**,\n",
        "#                 - 0 indicates the head is **masked**.\n",
        "\n",
        "#             past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "#                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
        "#                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
        "\n",
        "#                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
        "#                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "#                 If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
        "#                 that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
        "#                 all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "\n",
        "#             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "#                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
        "#                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
        "#                 than the model's internal embedding lookup matrix.\n",
        "#             output_attentions (`bool`, *optional*):\n",
        "#                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "#                 returned tensors for more detail.\n",
        "#             output_hidden_states (`bool`, *optional*):\n",
        "#                 Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "#                 for more detail.\n",
        "#             return_dict (`bool`, *optional*):\n",
        "#                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "#         \"\"\"\n",
        "#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "#         output_hidden_states = (\n",
        "#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "#         )\n",
        "#         use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "\n",
        "#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "#         # retrieve input_ids and inputs_embeds\n",
        "#         if input_ids is not None and inputs_embeds is not None:\n",
        "#             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
        "#         elif input_ids is not None:\n",
        "#             input_shape = input_ids.size()\n",
        "#             input_ids = input_ids.view(-1, input_shape[-1])\n",
        "#         elif inputs_embeds is not None:\n",
        "#             input_shape = inputs_embeds.size()[:-1]\n",
        "#         else:\n",
        "#             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
        "\n",
        "#         if inputs_embeds is None:\n",
        "#             inputs_embeds = self.embed_tokens(input_ids)\n",
        "\n",
        "#         batch_size, seq_length = input_shape\n",
        "#         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "#         # required mask seq length can be calculated via length of past\n",
        "#         mask_seq_length = past_key_values_length + seq_length\n",
        "\n",
        "#         # embed positions\n",
        "#         if attention_mask is None:\n",
        "#             attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n",
        "#         elif attention_mask.shape[1] != mask_seq_length:\n",
        "#             raise ValueError(\n",
        "#                 f\"The provided attention mask has length {attention_mask.shape[1]}, but its length should be \"\n",
        "#                 f\"{mask_seq_length} (sum of the lengths of current and past inputs)\"\n",
        "#             )\n",
        "#         causal_attention_mask = self._prepare_decoder_attention_mask(\n",
        "#             attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "#         )\n",
        "#         pos_embeds = self.embed_positions(attention_mask, past_key_values_length)\n",
        "\n",
        "#         if self.project_in is not None:\n",
        "#             inputs_embeds = self.project_in(inputs_embeds)\n",
        "\n",
        "#         hidden_states = inputs_embeds + pos_embeds\n",
        "\n",
        "#         if self.gradient_checkpointing and self.training:\n",
        "#             if use_cache:\n",
        "#                 logger.warning_once(\n",
        "#                     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "#                 )\n",
        "#                 use_cache = False\n",
        "\n",
        "#         # decoder layers\n",
        "#         all_hidden_states = () if output_hidden_states else None\n",
        "#         all_self_attns = () if output_attentions else None\n",
        "#         next_decoder_cache = () if use_cache else None\n",
        "\n",
        "#         # check if head_mask has a correct number of layers specified if desired\n",
        "#         for attn_mask, mask_name in zip([head_mask], [\"head_mask\"]):\n",
        "#             if attn_mask is not None:\n",
        "#                 if attn_mask.size()[0] != (len(self.layers)):\n",
        "#                     raise ValueError(\n",
        "#                         f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
        "#                         f\" {head_mask.size()[0]}.\"\n",
        "#                     )\n",
        "\n",
        "#         for idx, decoder_layer in enumerate(self.layers):\n",
        "#             # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "#             if output_hidden_states:\n",
        "#                 all_hidden_states += (hidden_states,)\n",
        "\n",
        "#             dropout_probability = random.uniform(0, 1)\n",
        "#             if self.training and (dropout_probability < self.layerdrop):\n",
        "#                 continue\n",
        "\n",
        "#             past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "#             if self.gradient_checkpointing and self.training:\n",
        "\n",
        "#                 def create_custom_forward(module):\n",
        "#                     def custom_forward(*inputs):\n",
        "#                         # None for past_key_value\n",
        "#                         return module(*inputs, output_attentions, None)\n",
        "\n",
        "#                     return custom_forward\n",
        "\n",
        "#                 layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "#                     create_custom_forward(decoder_layer),\n",
        "#                     hidden_states,\n",
        "#                     causal_attention_mask,\n",
        "#                     head_mask[idx] if head_mask is not None else None,\n",
        "#                     None,\n",
        "#                 )\n",
        "#             else:\n",
        "#                 layer_outputs = decoder_layer(\n",
        "#                     hidden_states,\n",
        "#                     attention_mask=causal_attention_mask,\n",
        "#                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "#                     past_key_value=past_key_value,\n",
        "#                     output_attentions=output_attentions,\n",
        "#                     use_cache=use_cache,\n",
        "#                 )\n",
        "\n",
        "#             hidden_states = layer_outputs[0]\n",
        "\n",
        "#             if use_cache:\n",
        "#                 next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
        "\n",
        "#             if output_attentions:\n",
        "#                 all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "#         if self.final_layer_norm is not None:\n",
        "#             hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "#         if self.project_out is not None:\n",
        "#             hidden_states = self.project_out(hidden_states)\n",
        "\n",
        "#         # add hidden states from the last decoder layer\n",
        "#         if output_hidden_states:\n",
        "#             all_hidden_states += (hidden_states,)\n",
        "\n",
        "#         next_cache = next_decoder_cache if use_cache else None\n",
        "#         if not return_dict:\n",
        "#             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
        "#         return BaseModelOutputWithPast(\n",
        "#             last_hidden_state=hidden_states,\n",
        "#             past_key_values=next_cache,\n",
        "#             hidden_states=all_hidden_states,\n",
        "#             attentions=all_self_attns,\n",
        "#         )\n",
        "\n",
        "\n",
        "# @add_start_docstrings(\n",
        "#     \"The bare OPT Model outputting raw hidden-states without any specific head on top.\",\n",
        "#     OPT_START_DOCSTRING,\n",
        "# )\n",
        "# class OPTModel(OPTPreTrainedModel):\n",
        "#     def __init__(self, config: OPTConfig):\n",
        "#         super().__init__(config)\n",
        "#         self.decoder = OPTDecoder(config)\n",
        "#         # Initialize weights and apply final processing\n",
        "#         self.post_init()\n",
        "\n",
        "#     def get_input_embeddings(self):\n",
        "#         return self.decoder.embed_tokens\n",
        "\n",
        "#     def set_input_embeddings(self, value):\n",
        "#         self.decoder.embed_tokens = value\n",
        "\n",
        "#     def get_decoder(self):\n",
        "#         return self.decoder\n",
        "\n",
        "#     @add_start_docstrings_to_model_forward(OPT_INPUTS_DOCSTRING)\n",
        "#     @add_code_sample_docstrings(\n",
        "#         checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "#         output_type=BaseModelOutputWithPast,\n",
        "#         config_class=_CONFIG_FOR_DOC,\n",
        "#         expected_output=_EXPECTED_OUTPUT_SHAPE,\n",
        "#     )\n",
        "#     def forward(\n",
        "#             self,\n",
        "#             input_ids: torch.LongTensor = None,\n",
        "#             attention_mask: Optional[torch.Tensor] = None,\n",
        "#             head_mask: Optional[torch.Tensor] = None,\n",
        "#             past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "#             inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "#             use_cache: Optional[bool] = None,\n",
        "#             output_attentions: Optional[bool] = None,\n",
        "#             output_hidden_states: Optional[bool] = None,\n",
        "#             return_dict: Optional[bool] = None,\n",
        "#     ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
        "#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "#         output_hidden_states = (\n",
        "#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "#         )\n",
        "#         use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "#         # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "#         decoder_outputs = self.decoder(\n",
        "#             input_ids=input_ids,\n",
        "#             attention_mask=attention_mask,\n",
        "#             head_mask=head_mask,\n",
        "#             past_key_values=past_key_values,\n",
        "#             inputs_embeds=inputs_embeds,\n",
        "#             use_cache=use_cache,\n",
        "#             output_attentions=output_attentions,\n",
        "#             output_hidden_states=output_hidden_states,\n",
        "#             return_dict=return_dict,\n",
        "#         )\n",
        "\n",
        "#         if not return_dict:\n",
        "#             return decoder_outputs\n",
        "\n",
        "#         return BaseModelOutputWithPast(\n",
        "#             last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "#             past_key_values=decoder_outputs.past_key_values,\n",
        "#             hidden_states=decoder_outputs.hidden_states,\n",
        "#             attentions=decoder_outputs.attentions,\n",
        "#         )\n",
        "\n",
        "\n",
        "# class OPTForCausalLM(OPTPreTrainedModel):\n",
        "#     _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n",
        "\n",
        "#     def __init__(self, config):\n",
        "#         super().__init__(config)\n",
        "#         self.model = OPTModel(config)\n",
        "\n",
        "#         # the lm_head weight is automatically tied to the embed tokens weight\n",
        "#         self.lm_head = nn.Linear(config.word_embed_proj_dim, config.vocab_size, bias=False)\n",
        "\n",
        "#         # Initialize weights and apply final processing\n",
        "#         self.post_init()\n",
        "\n",
        "#     def get_input_embeddings(self):\n",
        "#         return self.model.decoder.embed_tokens\n",
        "\n",
        "#     def set_input_embeddings(self, value):\n",
        "#         self.model.decoder.embed_tokens = value\n",
        "\n",
        "#     def get_output_embeddings(self):\n",
        "#         return self.lm_head\n",
        "\n",
        "#     def set_output_embeddings(self, new_embeddings):\n",
        "#         self.lm_head = new_embeddings\n",
        "\n",
        "#     def set_decoder(self, decoder):\n",
        "#         self.model.decoder = decoder\n",
        "\n",
        "#     def get_decoder(self):\n",
        "#         return self.model.decoder\n",
        "\n",
        "#     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
        "#     def forward(\n",
        "#             self,\n",
        "#             input_ids: torch.LongTensor = None,\n",
        "#             attention_mask: Optional[torch.Tensor] = None,\n",
        "#             head_mask: Optional[torch.Tensor] = None,\n",
        "#             past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "#             inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "#             labels: Optional[torch.LongTensor] = None,\n",
        "#             use_cache: Optional[bool] = None,\n",
        "#             output_attentions: Optional[bool] = None,\n",
        "#             output_hidden_states: Optional[bool] = None,\n",
        "#             return_dict: Optional[bool] = None,\n",
        "#     ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "#         r\"\"\"\n",
        "#         Args:\n",
        "#             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "#                 Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "#                 provide it.\n",
        "\n",
        "#                 Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "#                 [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "#                 [What are input IDs?](../glossary#input-ids)\n",
        "#             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "#                 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "#                 - 1 for tokens that are **not masked**,\n",
        "#                 - 0 for tokens that are **masked**.\n",
        "\n",
        "#                 [What are attention masks?](../glossary#attention-mask)\n",
        "#             head_mask (`torch.Tensor` of shape `(num_hidden_layers, num_attention_heads)`, *optional*):\n",
        "#                 Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
        "\n",
        "#                 - 1 indicates the head is **not masked**,\n",
        "#                 - 0 indicates the head is **masked**.\n",
        "\n",
        "#             past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "#                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
        "#                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
        "#                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n",
        "#                 tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n",
        "\n",
        "#                 Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
        "#                 cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "#                 If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
        "#                 that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
        "#                 all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "#             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "#                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
        "#                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
        "#                 than the model's internal embedding lookup matrix.\n",
        "#             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "#                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
        "#                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
        "#                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
        "#             use_cache (`bool`, *optional*):\n",
        "#                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
        "#                 (see `past_key_values`).\n",
        "#             output_attentions (`bool`, *optional*):\n",
        "#                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "#                 returned tensors for more detail.\n",
        "#             output_hidden_states (`bool`, *optional*):\n",
        "#                 Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "#                 for more detail.\n",
        "#             return_dict (`bool`, *optional*):\n",
        "#                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\n",
        "#         Returns:\n",
        "\n",
        "#         Example:\n",
        "\n",
        "#         ```python\n",
        "#         >>> from transformers import AutoTokenizer, OPTForCausalLM\n",
        "\n",
        "#         >>> model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "#         >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "#         >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
        "#         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "#         >>> # Generate\n",
        "#         >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
        "#         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "#         \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n",
        "#         ```\"\"\"\n",
        "\n",
        "#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "#         output_hidden_states = (\n",
        "#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "#         )\n",
        "#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "#         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "#         outputs = self.model.decoder(\n",
        "#             input_ids=input_ids,\n",
        "#             attention_mask=attention_mask,\n",
        "#             head_mask=head_mask,\n",
        "#             past_key_values=past_key_values,\n",
        "#             inputs_embeds=inputs_embeds,\n",
        "#             use_cache=use_cache,\n",
        "#             output_attentions=output_attentions,\n",
        "#             output_hidden_states=output_hidden_states,\n",
        "#             return_dict=return_dict,\n",
        "#         )\n",
        "\n",
        "#         logits = self.lm_head(outputs[0]).contiguous()\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             # move labels to correct device to enable model parallelism\n",
        "#             labels = labels.to(logits.device)\n",
        "#             # Shift so that tokens < n predict n\n",
        "#             shift_logits = logits[..., :-1, :].contiguous()\n",
        "#             shift_labels = labels[..., 1:].contiguous()\n",
        "#             # Flatten the tokens\n",
        "#             loss_fct = CrossEntropyLoss()\n",
        "#             loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
        "\n",
        "#         if not return_dict:\n",
        "#             output = (logits,) + outputs[1:]\n",
        "#             return (loss,) + output if loss is not None else output\n",
        "\n",
        "#         return CausalLMOutputWithPast(\n",
        "#             loss=loss,\n",
        "#             logits=logits,\n",
        "#             past_key_values=outputs.past_key_values,\n",
        "#             hidden_states=outputs.hidden_states,\n",
        "#             attentions=outputs.attentions,\n",
        "#         )\n",
        "\n",
        "#     def prepare_inputs_for_generation(\n",
        "#             self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
        "#     ):\n",
        "#         if past_key_values:\n",
        "#             input_ids = input_ids[:, -1:]\n",
        "\n",
        "#         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
        "#         if inputs_embeds is not None and past_key_values is None:\n",
        "#             model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
        "#         else:\n",
        "#             model_inputs = {\"input_ids\": input_ids}\n",
        "\n",
        "#         model_inputs.update(\n",
        "#             {\n",
        "#                 \"past_key_values\": past_key_values,\n",
        "#                 \"use_cache\": kwargs.get(\"use_cache\"),\n",
        "#                 \"attention_mask\": attention_mask,\n",
        "#             }\n",
        "#         )\n",
        "#         return model_inputs\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _reorder_cache(past_key_values, beam_idx):\n",
        "#         reordered_past = ()\n",
        "#         for layer_past in past_key_values:\n",
        "#             reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "#         return reordered_past\n",
        "\n",
        "\n",
        "# @add_start_docstrings(\n",
        "#     \"\"\"\n",
        "#     The OPT Model transformer with a sequence classification head on top (linear layer).\n",
        "\n",
        "#     [`OPTForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n",
        "#     (e.g. GPT-2) do.\n",
        "\n",
        "#     Since it does classification on the last token, it requires to know the position of the last token. If a\n",
        "#     `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n",
        "#     no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n",
        "#     padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n",
        "#     each row of the batch).\n",
        "#     \"\"\",\n",
        "#     OPT_START_DOCSTRING,\n",
        "# )\n",
        "# class OPTForSequenceClassification(OPTPreTrainedModel):\n",
        "#     _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n",
        "\n",
        "#     def __init__(self, config: OPTConfig):\n",
        "#         super().__init__(config)\n",
        "#         self.num_labels = config.num_labels\n",
        "#         self.model = OPTModel(config)\n",
        "#         self.score = nn.Linear(config.word_embed_proj_dim, self.num_labels, bias=False)\n",
        "\n",
        "#         # Initialize weights and apply final processing\n",
        "#         self.post_init()\n",
        "\n",
        "#     @add_start_docstrings_to_model_forward(OPT_INPUTS_DOCSTRING)\n",
        "#     @add_code_sample_docstrings(\n",
        "#         checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n",
        "#         output_type=SequenceClassifierOutputWithPast,\n",
        "#         config_class=_CONFIG_FOR_DOC,\n",
        "#         expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n",
        "#         expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n",
        "#     )\n",
        "#     def forward(\n",
        "#             self,\n",
        "#             input_ids: Optional[torch.LongTensor] = None,\n",
        "#             attention_mask: Optional[torch.FloatTensor] = None,\n",
        "#             head_mask: Optional[torch.FloatTensor] = None,\n",
        "#             past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "#             inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "#             labels: Optional[torch.LongTensor] = None,\n",
        "#             use_cache: Optional[bool] = None,\n",
        "#             output_attentions: Optional[bool] = None,\n",
        "#             output_hidden_states: Optional[bool] = None,\n",
        "#             return_dict: Optional[bool] = None,\n",
        "#     ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
        "#         r\"\"\"\n",
        "#         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "#             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "#             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "#             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "#         \"\"\"\n",
        "#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "#         transformer_outputs = self.model(\n",
        "#             input_ids,\n",
        "#             past_key_values=past_key_values,\n",
        "#             attention_mask=attention_mask,\n",
        "#             head_mask=head_mask,\n",
        "#             inputs_embeds=inputs_embeds,\n",
        "#             use_cache=use_cache,\n",
        "#             output_attentions=output_attentions,\n",
        "#             output_hidden_states=output_hidden_states,\n",
        "#             return_dict=return_dict,\n",
        "#         )\n",
        "#         hidden_states = transformer_outputs[0]\n",
        "#         logits = self.score(hidden_states)\n",
        "\n",
        "#         if input_ids is not None:\n",
        "#             batch_size, sequence_length = input_ids.shape[:2]\n",
        "#         else:\n",
        "#             batch_size, sequence_length = inputs_embeds.shape[:2]\n",
        "\n",
        "#         if self.config.pad_token_id is None:\n",
        "#             sequence_lengths = -1\n",
        "#         else:\n",
        "#             if input_ids is not None:\n",
        "#                 sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n",
        "#             else:\n",
        "#                 sequence_lengths = -1\n",
        "#                 logger.warning(\n",
        "#                     f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
        "#                     \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
        "#                 )\n",
        "\n",
        "#         pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
        "\n",
        "#         loss = None\n",
        "#         if labels is not None:\n",
        "#             if self.config.problem_type is None:\n",
        "#                 if self.num_labels == 1:\n",
        "#                     self.config.problem_type = \"regression\"\n",
        "#                 elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "#                     self.config.problem_type = \"single_label_classification\"\n",
        "#                 else:\n",
        "#                     self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "#             if self.config.problem_type == \"regression\":\n",
        "#                 loss_fct = MSELoss()\n",
        "#                 if self.num_labels == 1:\n",
        "#                     loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n",
        "#                 else:\n",
        "#                     loss = loss_fct(pooled_logits, labels)\n",
        "#             elif self.config.problem_type == \"single_label_classification\":\n",
        "#                 loss_fct = CrossEntropyLoss()\n",
        "#                 loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "#             elif self.config.problem_type == \"multi_label_classification\":\n",
        "#                 loss_fct = BCEWithLogitsLoss()\n",
        "#                 loss = loss_fct(pooled_logits, labels)\n",
        "#         if not return_dict:\n",
        "#             output = (pooled_logits,) + transformer_outputs[1:]\n",
        "#             return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "#         return SequenceClassifierOutputWithPast(\n",
        "#             loss=loss,\n",
        "#             logits=pooled_logits,\n",
        "#             past_key_values=transformer_outputs.past_key_values,\n",
        "#             hidden_states=transformer_outputs.hidden_states,\n",
        "#             attentions=transformer_outputs.attentions,\n",
        "#         )\n",
        "\n",
        "#     def get_input_embeddings(self):\n",
        "#         return self.model.decoder.embed_tokens\n",
        "\n",
        "#     def set_input_embeddings(self, value):\n",
        "#         self.model.decoder.embed_tokens = value\n",
        "\n",
        "\n",
        "# @add_start_docstrings(\n",
        "#     \"\"\"\n",
        "#     The OPT Model transformer with a span classification head on top for extractive question-answering tasks like SQuAD\n",
        "#     (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "#     \"\"\",\n",
        "#     OPT_START_DOCSTRING,\n",
        "# )\n",
        "# class OPTForQuestionAnswering(OPTPreTrainedModel):\n",
        "#     _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n",
        "\n",
        "#     def __init__(self, config: OPTConfig):\n",
        "#         super().__init__(config)\n",
        "#         self.model = OPTModel(config)\n",
        "#         self.qa_outputs = nn.Linear(config.word_embed_proj_dim, 2)\n",
        "\n",
        "#         # Initialize weights and apply final processing\n",
        "#         self.post_init()\n",
        "\n",
        "#     @add_start_docstrings_to_model_forward(OPT_INPUTS_DOCSTRING)\n",
        "#     @replace_return_docstrings(output_type=QuestionAnsweringModelOutput, config_class=_CONFIG_FOR_DOC)\n",
        "#     def forward(\n",
        "#             self,\n",
        "#             input_ids: Optional[torch.LongTensor] = None,\n",
        "#             attention_mask: Optional[torch.FloatTensor] = None,\n",
        "#             head_mask: Optional[torch.FloatTensor] = None,\n",
        "#             past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "#             inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "#             start_positions: Optional[torch.LongTensor] = None,\n",
        "#             end_positions: Optional[torch.LongTensor] = None,\n",
        "#             use_cache: Optional[bool] = None,\n",
        "#             output_attentions: Optional[bool] = None,\n",
        "#             output_hidden_states: Optional[bool] = None,\n",
        "#             return_dict: Optional[bool] = None,\n",
        "#     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n",
        "#         r\"\"\"\n",
        "#         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "#             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "#             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
        "#             are not taken into account for computing the loss.\n",
        "#         end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "#             Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "#             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n",
        "#             are not taken into account for computing the loss.\n",
        "\n",
        "#         Returns:\n",
        "\n",
        "#         Example:\n",
        "\n",
        "#         ```python\n",
        "#         >>> from transformers import AutoTokenizer, OPTForQuestionAnswering\n",
        "#         >>> import torch\n",
        "\n",
        "#         >>> torch.manual_seed(4)  # doctest: +IGNORE_RESULT\n",
        "#         >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "#         >>> # note: we are loading a OPTForQuestionAnswering from the hub here,\n",
        "#         >>> # so the head will be randomly initialized, hence the predictions will be random\n",
        "#         >>> model = OPTForQuestionAnswering.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "#         >>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
        "\n",
        "#         >>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
        "#         >>> with torch.no_grad():\n",
        "#         ...     outputs = model(**inputs)\n",
        "\n",
        "#         >>> answer_start_index = outputs.start_logits.argmax()\n",
        "#         >>> answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "#         >>> answer_offset = len(tokenizer(question)[0])\n",
        "\n",
        "#         >>> predict_answer_tokens = inputs.input_ids[\n",
        "#         ...     0, answer_offset + answer_start_index : answer_offset + answer_end_index + 1\n",
        "#         ... ]\n",
        "#         >>> predicted = tokenizer.decode(predict_answer_tokens)\n",
        "#         >>> predicted\n",
        "#         ' a nice puppet'\n",
        "#         ```\"\"\"\n",
        "#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "#         transformer_outputs = self.model(\n",
        "#             input_ids,\n",
        "#             past_key_values=past_key_values,\n",
        "#             attention_mask=attention_mask,\n",
        "#             head_mask=head_mask,\n",
        "#             inputs_embeds=inputs_embeds,\n",
        "#             use_cache=use_cache,\n",
        "#             output_attentions=output_attentions,\n",
        "#             output_hidden_states=output_hidden_states,\n",
        "#             return_dict=return_dict,\n",
        "#         )\n",
        "#         hidden_states = transformer_outputs[0]\n",
        "\n",
        "#         logits = self.qa_outputs(hidden_states)\n",
        "#         start_logits, end_logits = logits.split(1, dim=-1)\n",
        "#         start_logits = start_logits.squeeze(-1).contiguous()\n",
        "#         end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "#         total_loss = None\n",
        "#         if start_positions is not None and end_positions is not None:\n",
        "#             # If we are on multi-GPU, split add a dimension\n",
        "#             if len(start_positions.size()) > 1:\n",
        "#                 start_positions = start_positions.squeeze(-1)\n",
        "#             if len(end_positions.size()) > 1:\n",
        "#                 end_positions = end_positions.squeeze(-1)\n",
        "#             # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "#             ignored_index = start_logits.size(1)\n",
        "#             start_positions = start_positions.clamp(0, ignored_index)\n",
        "#             end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "#             loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "#             start_loss = loss_fct(start_logits, start_positions)\n",
        "#             end_loss = loss_fct(end_logits, end_positions)\n",
        "#             total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "#         if not return_dict:\n",
        "#             output = (start_logits, end_logits) + transformer_outputs[2:]\n",
        "#             return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "#         return QuestionAnsweringModelOutput(\n",
        "#             loss=total_loss,\n",
        "#             start_logits=start_logits,\n",
        "#             end_logits=end_logits,\n",
        "#             hidden_states=transformer_outputs.hidden_states,\n",
        "#             attentions=transformer_outputs.attentions,\n",
        "#         )\n",
        "\n",
        "#     def get_input_embeddings(self):\n",
        "#         return self.model.decoder.embed_tokens\n",
        "\n",
        "#     def set_input_embeddings(self, value):\n",
        "#         self.model.decoder.embed_tokens = value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1NLMeSQA00T"
      },
      "source": [
        "# Task1: Implement LoRA in the OPTAttention Class\n",
        "\n",
        "In this task, you will enhance the `OPTAttention` class by integrating Low-Rank Adaptation (LoRA) into the multi-headed attention mechanism. This modification aims to improve the model's adaptability and efficiency, making it more suitable for fine-tuning purposes.\n",
        "\n",
        "#### Requirements:\n",
        "1. Understand the structure and functionality of the multi-headed attention mechanism as introduced in the \"Attention Is All You Need\" paper.\n",
        "2. Gain familiarity with the concept of Low-Rank Adaptation (LoRA) and how it can be applied to neural network layers, specifically within the context of attention mechanisms.\n",
        "\n",
        "#### Instructions:\n",
        "\n",
        "1. **LoRA Parameters Initialization**:\n",
        "    - In the `__init__` method of the `OPTAttention` class, initialize the LoRA parameters `lora_A_k`, `lora_B_k`, `lora_A_v`, and `lora_B_v` following the standard attention parameters initialization.\n",
        "    - These parameters introduce low-rank matrices that will modify the standard key and value projections within the attention mechanism. The matrices `lora_A_k` and `lora_A_v` reduce the dimension from the embedding dimension to a smaller rank, while `lora_B_k` and `lora_B_v` project them back to the original space.\n",
        "\n",
        "2. **Implementing LoRA in Attention Projections**:\n",
        "    - Within the `forward` method, integrate the LoRA parameters with the attention mechanism. Specifically, modify the `key_states` and `value_states` calculations where indicated by the comments.\n",
        "    - Apply LoRA adaptation by combining the original key and value states (obtained from the standard projections) with the outcomes of passing `key_value_states` or `hidden_states` through the LoRA matrices.\n",
        "    - Ensure proper tensor shapes and dimensions when combining LoRA-adapted projections with the standard projections to ensure model consistency and correctness.\n",
        "\n",
        "3. **Scaling and Regularization with LoRA**:\n",
        "    - Apply `lora_scaling` to the LoRA-adapted components before adding them to the standard key and value projections. `lora_scaling` controls the impact of LoRA modifications on the model and is computed as `lora_alpha / rank`, where `lora_alpha` is a predefined scaling factor and `rank` is the dimensionality reduction factor.\n",
        "    - Implement `lora_dropout` by applying dropout to the reduced-dimension representations in the LoRA layers. This helps in regularizing the LoRA modifications and preventing overfitting.\n",
        "\n",
        "4. **Validation**:\n",
        "    - Validate your implementation by ensuring it passes any provided tests or validation checks. Pay particular attention to the tensor shapes and the logical flow of your LoRA modifications to ensure they align with the expected functionality of the multi-headed attention mechanism.\n",
        "\n",
        "#### Example Implementation:\n",
        "Below is an example of how you might modify a section of the code for LoRA integration within the attention projections:\n",
        "\n",
        "```python\n",
        "# Original line for context:\n",
        "key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "\n",
        "# Modified line with LoRA:\n",
        "key_states = self._shape(\n",
        "    # The LoRA Operation, please refer to the paper for more detail\n",
        ")\n",
        "#Please aware of the comment with ###\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:15.283102Z",
          "iopub.status.busy": "2024-03-10T12:06:15.282817Z",
          "iopub.status.idle": "2024-03-10T12:06:15.315264Z",
          "shell.execute_reply": "2024-03-10T12:06:15.314368Z",
          "shell.execute_reply.started": "2024-03-10T12:06:15.283078Z"
        },
        "id": "-tuiWOBKA0XL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# class OPTAttention(nn.Module):\n",
        "#     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "#     def __init__(\n",
        "#             self,\n",
        "#             embed_dim: int,\n",
        "#             num_heads: int,\n",
        "#             dropout: float = 0.0,\n",
        "#             is_decoder: bool = False,\n",
        "#             bias: bool = True,\n",
        "#     ):\n",
        "#         super().__init__()\n",
        "#         self.embed_dim = embed_dim\n",
        "#         self.num_heads = num_heads\n",
        "#         self.dropout = dropout\n",
        "#         self.head_dim = embed_dim // num_heads\n",
        "\n",
        "#         if (self.head_dim * num_heads) != self.embed_dim:\n",
        "#             raise ValueError(\n",
        "#                 f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
        "#                 f\" and `num_heads`: {num_heads}).\"\n",
        "#             )\n",
        "#         self.scaling = self.head_dim ** -0.5\n",
        "#         self.is_decoder = is_decoder\n",
        "\n",
        "#         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "#         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "#         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "#         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "#         ### Your code here:\n",
        "#         rank = 16\n",
        "#         lora_alpha = 32\n",
        "#         lora_dropout = 0.05\n",
        "#         self.lora_A_k = nn.Linear(self.embed_dim, rank, bias=False) #input output matrix\n",
        "#         self.lora_B_k = nn.Linear(rank, self.embed_dim, bias=False)\n",
        "#         self.lora_A_v = nn.Linear(self.embed_dim, rank, bias=False)\n",
        "#         self.lora_B_v = nn.Linear(rank, self.embed_dim, bias=False)\n",
        "#         self.lora_scaling = lora_alpha/rank\n",
        "#         self.lora_dropout = nn.Dropout(lora_dropout)\n",
        "#         ### End of code writing\n",
        "\n",
        "#     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "#         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "#     def forward(\n",
        "#             self,\n",
        "#             hidden_states: torch.Tensor,\n",
        "#             key_value_states: Optional[torch.Tensor] = None,\n",
        "#             past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "#             attention_mask: Optional[torch.Tensor] = None,\n",
        "#             layer_head_mask: Optional[torch.Tensor] = None,\n",
        "#             output_attentions: bool = False,\n",
        "#     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "#         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "#         # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "#         # for the decoder\n",
        "#         is_cross_attention = key_value_states is not None\n",
        "\n",
        "#         bsz, tgt_len, _ = hidden_states.size()\n",
        "\n",
        "#         # get query proj\n",
        "#         query_states = self.q_proj(hidden_states) * self.scaling\n",
        "#         # get key, value proj\n",
        "#         if is_cross_attention and past_key_value is not None:\n",
        "#             # reuse k,v, cross_attentions\n",
        "#             key_states = past_key_value[0]\n",
        "#             value_states = past_key_value[1]\n",
        "#         elif is_cross_attention:\n",
        "#             # cross_attentions\n",
        "#             ### Your code here: add lora to key value state\n",
        "#             key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "#             value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "#         elif past_key_value is not None:\n",
        "#             # reuse k, v, self_attention\n",
        "#             ### Your code here: add lora to key value state\n",
        "#             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "#             value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "#             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "#             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "#         else:\n",
        "#             # self_attention\n",
        "#             ### Your code here: add lora to key value state\n",
        "#             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "#             value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "\n",
        "#         if self.is_decoder:\n",
        "#             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "#             # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "#             # key/value_states (first \"if\" case)\n",
        "#             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "#             # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "#             # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "#             # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "#             past_key_value = (key_states, value_states)\n",
        "\n",
        "#         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "#         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "#         key_states = key_states.view(*proj_shape)\n",
        "#         value_states = value_states.view(*proj_shape)\n",
        "\n",
        "#         src_len = key_states.size(1)\n",
        "#         attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "#         if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
        "#             raise ValueError(\n",
        "#                 f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
        "#                 f\" {attn_weights.size()}\"\n",
        "#             )\n",
        "\n",
        "#         if attention_mask is not None:\n",
        "#             if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
        "#                 raise ValueError(\n",
        "#                     f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "#                 )\n",
        "#             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "#             attn_weights = torch.max(\n",
        "#                 attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n",
        "#             )\n",
        "#             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "#         # upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\n",
        "#         if attn_weights.dtype == torch.float16:\n",
        "#             attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)\n",
        "#         else:\n",
        "#             attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "#         if layer_head_mask is not None:\n",
        "#             if layer_head_mask.size() != (self.num_heads,):\n",
        "#                 raise ValueError(\n",
        "#                     f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
        "#                     f\" {layer_head_mask.size()}\"\n",
        "#                 )\n",
        "#             attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "#             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "#         if output_attentions:\n",
        "#             # this operation is a bit awkward, but it's required to\n",
        "#             # make sure that attn_weights keeps its gradient.\n",
        "#             # In order to do so, attn_weights have to be reshaped\n",
        "#             # twice and have to be reused in the following\n",
        "#             attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "#             attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "#         else:\n",
        "#             attn_weights_reshaped = None\n",
        "\n",
        "#         attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "#         attn_output = torch.bmm(attn_probs, value_states)\n",
        "\n",
        "#         if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
        "#             raise ValueError(\n",
        "#                 f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
        "#                 f\" {attn_output.size()}\"\n",
        "#             )\n",
        "\n",
        "#         attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "#         attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "#         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
        "#         # partitioned aross GPUs when using tensor-parallelism.\n",
        "#         attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
        "\n",
        "#         attn_output = self.out_proj(attn_output)\n",
        "\n",
        "#         return attn_output, attn_weights_reshaped, past_key_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0lY-JaeCOvM"
      },
      "source": [
        "# Initialize the model\n",
        "- In this tutorial, we use OPT-2.7B as the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:15.316882Z",
          "iopub.status.busy": "2024-03-10T12:06:15.316554Z",
          "iopub.status.idle": "2024-03-10T12:06:38.223423Z",
          "shell.execute_reply": "2024-03-10T12:06:38.222403Z",
          "shell.execute_reply.started": "2024-03-10T12:06:15.316858Z"
        },
        "id": "gHu_vmH9CF_H",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of OPTForCausalLM were not initialized from the model checkpoint at facebook/opt-2.7b and are newly initialized: ['model.decoder.layers.0.self_attn.lora_A_k.weight', 'model.decoder.layers.0.self_attn.lora_A_v.weight', 'model.decoder.layers.0.self_attn.lora_B_k.weight', 'model.decoder.layers.0.self_attn.lora_B_v.weight', 'model.decoder.layers.1.self_attn.lora_A_k.weight', 'model.decoder.layers.1.self_attn.lora_A_v.weight', 'model.decoder.layers.1.self_attn.lora_B_k.weight', 'model.decoder.layers.1.self_attn.lora_B_v.weight', 'model.decoder.layers.10.self_attn.lora_A_k.weight', 'model.decoder.layers.10.self_attn.lora_A_v.weight', 'model.decoder.layers.10.self_attn.lora_B_k.weight', 'model.decoder.layers.10.self_attn.lora_B_v.weight', 'model.decoder.layers.11.self_attn.lora_A_k.weight', 'model.decoder.layers.11.self_attn.lora_A_v.weight', 'model.decoder.layers.11.self_attn.lora_B_k.weight', 'model.decoder.layers.11.self_attn.lora_B_v.weight', 'model.decoder.layers.12.self_attn.lora_A_k.weight', 'model.decoder.layers.12.self_attn.lora_A_v.weight', 'model.decoder.layers.12.self_attn.lora_B_k.weight', 'model.decoder.layers.12.self_attn.lora_B_v.weight', 'model.decoder.layers.13.self_attn.lora_A_k.weight', 'model.decoder.layers.13.self_attn.lora_A_v.weight', 'model.decoder.layers.13.self_attn.lora_B_k.weight', 'model.decoder.layers.13.self_attn.lora_B_v.weight', 'model.decoder.layers.14.self_attn.lora_A_k.weight', 'model.decoder.layers.14.self_attn.lora_A_v.weight', 'model.decoder.layers.14.self_attn.lora_B_k.weight', 'model.decoder.layers.14.self_attn.lora_B_v.weight', 'model.decoder.layers.15.self_attn.lora_A_k.weight', 'model.decoder.layers.15.self_attn.lora_A_v.weight', 'model.decoder.layers.15.self_attn.lora_B_k.weight', 'model.decoder.layers.15.self_attn.lora_B_v.weight', 'model.decoder.layers.16.self_attn.lora_A_k.weight', 'model.decoder.layers.16.self_attn.lora_A_v.weight', 'model.decoder.layers.16.self_attn.lora_B_k.weight', 'model.decoder.layers.16.self_attn.lora_B_v.weight', 'model.decoder.layers.17.self_attn.lora_A_k.weight', 'model.decoder.layers.17.self_attn.lora_A_v.weight', 'model.decoder.layers.17.self_attn.lora_B_k.weight', 'model.decoder.layers.17.self_attn.lora_B_v.weight', 'model.decoder.layers.18.self_attn.lora_A_k.weight', 'model.decoder.layers.18.self_attn.lora_A_v.weight', 'model.decoder.layers.18.self_attn.lora_B_k.weight', 'model.decoder.layers.18.self_attn.lora_B_v.weight', 'model.decoder.layers.19.self_attn.lora_A_k.weight', 'model.decoder.layers.19.self_attn.lora_A_v.weight', 'model.decoder.layers.19.self_attn.lora_B_k.weight', 'model.decoder.layers.19.self_attn.lora_B_v.weight', 'model.decoder.layers.2.self_attn.lora_A_k.weight', 'model.decoder.layers.2.self_attn.lora_A_v.weight', 'model.decoder.layers.2.self_attn.lora_B_k.weight', 'model.decoder.layers.2.self_attn.lora_B_v.weight', 'model.decoder.layers.20.self_attn.lora_A_k.weight', 'model.decoder.layers.20.self_attn.lora_A_v.weight', 'model.decoder.layers.20.self_attn.lora_B_k.weight', 'model.decoder.layers.20.self_attn.lora_B_v.weight', 'model.decoder.layers.21.self_attn.lora_A_k.weight', 'model.decoder.layers.21.self_attn.lora_A_v.weight', 'model.decoder.layers.21.self_attn.lora_B_k.weight', 'model.decoder.layers.21.self_attn.lora_B_v.weight', 'model.decoder.layers.22.self_attn.lora_A_k.weight', 'model.decoder.layers.22.self_attn.lora_A_v.weight', 'model.decoder.layers.22.self_attn.lora_B_k.weight', 'model.decoder.layers.22.self_attn.lora_B_v.weight', 'model.decoder.layers.23.self_attn.lora_A_k.weight', 'model.decoder.layers.23.self_attn.lora_A_v.weight', 'model.decoder.layers.23.self_attn.lora_B_k.weight', 'model.decoder.layers.23.self_attn.lora_B_v.weight', 'model.decoder.layers.24.self_attn.lora_A_k.weight', 'model.decoder.layers.24.self_attn.lora_A_v.weight', 'model.decoder.layers.24.self_attn.lora_B_k.weight', 'model.decoder.layers.24.self_attn.lora_B_v.weight', 'model.decoder.layers.25.self_attn.lora_A_k.weight', 'model.decoder.layers.25.self_attn.lora_A_v.weight', 'model.decoder.layers.25.self_attn.lora_B_k.weight', 'model.decoder.layers.25.self_attn.lora_B_v.weight', 'model.decoder.layers.26.self_attn.lora_A_k.weight', 'model.decoder.layers.26.self_attn.lora_A_v.weight', 'model.decoder.layers.26.self_attn.lora_B_k.weight', 'model.decoder.layers.26.self_attn.lora_B_v.weight', 'model.decoder.layers.27.self_attn.lora_A_k.weight', 'model.decoder.layers.27.self_attn.lora_A_v.weight', 'model.decoder.layers.27.self_attn.lora_B_k.weight', 'model.decoder.layers.27.self_attn.lora_B_v.weight', 'model.decoder.layers.28.self_attn.lora_A_k.weight', 'model.decoder.layers.28.self_attn.lora_A_v.weight', 'model.decoder.layers.28.self_attn.lora_B_k.weight', 'model.decoder.layers.28.self_attn.lora_B_v.weight', 'model.decoder.layers.29.self_attn.lora_A_k.weight', 'model.decoder.layers.29.self_attn.lora_A_v.weight', 'model.decoder.layers.29.self_attn.lora_B_k.weight', 'model.decoder.layers.29.self_attn.lora_B_v.weight', 'model.decoder.layers.3.self_attn.lora_A_k.weight', 'model.decoder.layers.3.self_attn.lora_A_v.weight', 'model.decoder.layers.3.self_attn.lora_B_k.weight', 'model.decoder.layers.3.self_attn.lora_B_v.weight', 'model.decoder.layers.30.self_attn.lora_A_k.weight', 'model.decoder.layers.30.self_attn.lora_A_v.weight', 'model.decoder.layers.30.self_attn.lora_B_k.weight', 'model.decoder.layers.30.self_attn.lora_B_v.weight', 'model.decoder.layers.31.self_attn.lora_A_k.weight', 'model.decoder.layers.31.self_attn.lora_A_v.weight', 'model.decoder.layers.31.self_attn.lora_B_k.weight', 'model.decoder.layers.31.self_attn.lora_B_v.weight', 'model.decoder.layers.4.self_attn.lora_A_k.weight', 'model.decoder.layers.4.self_attn.lora_A_v.weight', 'model.decoder.layers.4.self_attn.lora_B_k.weight', 'model.decoder.layers.4.self_attn.lora_B_v.weight', 'model.decoder.layers.5.self_attn.lora_A_k.weight', 'model.decoder.layers.5.self_attn.lora_A_v.weight', 'model.decoder.layers.5.self_attn.lora_B_k.weight', 'model.decoder.layers.5.self_attn.lora_B_v.weight', 'model.decoder.layers.6.self_attn.lora_A_k.weight', 'model.decoder.layers.6.self_attn.lora_A_v.weight', 'model.decoder.layers.6.self_attn.lora_B_k.weight', 'model.decoder.layers.6.self_attn.lora_B_v.weight', 'model.decoder.layers.7.self_attn.lora_A_k.weight', 'model.decoder.layers.7.self_attn.lora_A_v.weight', 'model.decoder.layers.7.self_attn.lora_B_k.weight', 'model.decoder.layers.7.self_attn.lora_B_v.weight', 'model.decoder.layers.8.self_attn.lora_A_k.weight', 'model.decoder.layers.8.self_attn.lora_A_v.weight', 'model.decoder.layers.8.self_attn.lora_B_k.weight', 'model.decoder.layers.8.self_attn.lora_B_v.weight', 'model.decoder.layers.9.self_attn.lora_A_k.weight', 'model.decoder.layers.9.self_attn.lora_A_v.weight', 'model.decoder.layers.9.self_attn.lora_B_k.weight', 'model.decoder.layers.9.self_attn.lora_B_v.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# # model_name = 'facebook/opt-2.7b'\n",
        "# # model = OPTForCausalLM.from_pretrained(model_name)\n",
        "# model_name = 'google/gemma-2b'\n",
        "# model = GemmaForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zes4OQZfCsz2"
      },
      "source": [
        "# Task2: Post-Initialization and Freezing Model Weights\n",
        "\n",
        "In this section, you will perform two critical steps necessary for preparing your model for LoRA-based fine-tuning: post-initialization of LoRA parameters and freezing the base model weights. Follow the instructions below to understand and implement these steps in your code.\n",
        "\n",
        "#### Post-Initialization of LoRA Parameters:\n",
        "After integrating LoRA into your model, it is crucial to initialize the parameters correctly:\n",
        "\n",
        "1. **Kaiming Uniform Initialization for LoRA_A**:\n",
        "   - Use the `nn.init.kaiming_uniform_` function to initialize parameters associated with `lora_A`.\n",
        "   - This type of initialization is well-suited for layers followed by ReLU activations, as it considers the size of the previous layer to maintain a consistent variance of activations.\n",
        "   - The parameter `a` is set based on the rectifier linear unit's negative slope, optimizing the initialization for layers that use ReLU or similar activations.\n",
        "\n",
        "2. **Zero Initialization for LoRA_B**:\n",
        "   - Initialize the `lora_B` parameters with zeros using `nn.init.zeros_`.\n",
        "   - This ensures that the LoRA modifications start from a neutral state, allowing the adapted parts of the model to learn from scratch during training.\n",
        "\n",
        "#### Freezing Base Model Weights:\n",
        "To focus the learning process on the LoRA parameters while preserving the pre-trained knowledge in other parts of the model, you need to freeze the base model weights:\n",
        "\n",
        "1. **Set `requires_grad` to False for Non-LoRA Parameters**:\n",
        "   - Iterate through all named parameters in the model. If a parameter's name does not contain 'lora', set its `requires_grad` attribute to `False`.\n",
        "   - This action prevents the standard backpropagation updates from modifying these parameters, effectively freezing them during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:38.225236Z",
          "iopub.status.busy": "2024-03-10T12:06:38.224916Z",
          "iopub.status.idle": "2024-03-10T12:06:38.268161Z",
          "shell.execute_reply": "2024-03-10T12:06:38.267370Z",
          "shell.execute_reply.started": "2024-03-10T12:06:38.225210Z"
        },
        "id": "UnVhkxiPAU-D",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Write your code following the instructions\n",
        "# do post initialization on lora A and lora B\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora_A' in name:\n",
        "        nn.init.kaiming_uniform_(param, a=math.sqrt(5))\n",
        "    elif 'lora_B' in name:\n",
        "        nn.init.zeros_(param)\n",
        "# freeze base model weight\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora' not in name:\n",
        "        param.require_grad=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:38.269625Z",
          "iopub.status.busy": "2024-03-10T12:06:38.269267Z",
          "iopub.status.idle": "2024-03-10T12:06:40.317132Z",
          "shell.execute_reply": "2024-03-10T12:06:40.316144Z",
          "shell.execute_reply.started": "2024-03-10T12:06:38.269594Z"
        },
        "id": "vFkvrMCaC7r-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# enable gradient checkpoint for lower gpu memory usage\n",
        "model.gradient_checkpointing_enable()\n",
        "# enable input gradient for lora training\n",
        "model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIGbE7feDMS1"
      },
      "source": [
        "# Monitoring Model Parameters\n",
        "\n",
        "In machine learning and deep learning, understanding the structure and capacity of your model is crucial. One key aspect of this is knowing the number of parameters that are being trained. This information can help in diagnosing model complexity, memory usage, and potential overfitting or underfitting scenarios.\n",
        "\n",
        "#### Function Overview: `print_trainable_parameters`\n",
        "This function is designed to provide a clear overview of the parameters within a given model. Specifically, it calculates and prints the total number of parameters, the number of trainable parameters, and the percentage of parameters that are trainable. Here's a breakdown of its functionalities:\n",
        "\n",
        "1. **Total Parameters Count (`all_param`)**: This represents the total number of parameters in the model, including both trainable and non-trainable (frozen) parameters.\n",
        "\n",
        "2. **Trainable Parameters Count (`trainable_params`)**: This is the subset of the total parameters that will be updated during training. Parameters that are not trainable have been frozen and will retain their values during the training process.\n",
        "\n",
        "3. **Trainable Percentage**: This metric provides insight into how much of the model is being actively trained. A lower percentage might indicate that a large portion of the model is frozen, which could be intentional in scenarios like transfer learning or fine-tuning.\n",
        "\n",
        "The function iterates through all parameters in the model, counting the total and trainable parameters, and then prints out these values along with the percentage of parameters that are trainable. This utility can be particularly useful when you are experimenting with different architectures or when fine-tuning pre-trained models.\n",
        "\n",
        "#### Usage:\n",
        "Simply call `print_trainable_parameters(model)` after defining your model. This will output the parameter statistics, giving you a better understanding of your model's capacity and training scope.\n",
        "\n",
        "Example output:\n",
        "```plaintext\n",
        "trainable params: 1024 || all params: 2048 || trainable%: 50.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:40.320853Z",
          "iopub.status.busy": "2024-03-10T12:06:40.320240Z",
          "iopub.status.idle": "2024-03-10T12:06:40.394815Z",
          "shell.execute_reply": "2024-03-10T12:06:40.393704Z",
          "shell.execute_reply.started": "2024-03-10T12:06:40.320817Z"
        },
        "id": "rOJBHBFMC86P",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "    return {\"trainable\": trainable_params, \"all\": all_param, \"trainable%\": 100 * trainable_params / all_param}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm4SKPuKDRai"
      },
      "source": [
        "# Trainable Parameter Check\n",
        "- Just to check how many parameters are trainable in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:40.397102Z",
          "iopub.status.busy": "2024-03-10T12:06:40.396264Z",
          "iopub.status.idle": "2024-03-10T12:06:40.467367Z",
          "shell.execute_reply": "2024-03-10T12:06:40.466285Z",
          "shell.execute_reply.started": "2024-03-10T12:06:40.397069Z"
        },
        "id": "YVtcseVtDQnS",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2621440 || all params: 2654218240 || trainable%: 0.09876505106075979\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'trainable': 2621440, 'all': 2654218240, 'trainable%': 0.09876505106075979}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE34EasAEFzL"
      },
      "source": [
        "# Task3: Implementing Custom Dataset for NLP\n",
        "\n",
        "In this task, you will create a custom dataset class, `HGDataset`, using PyTorch for a natural language processing (NLP) task. This custom dataset will facilitate data handling for training and testing NLP models. Follow the instructions below to complete the missing parts of the `HGDataset` class.\n",
        "\n",
        "#### Objectives:\n",
        "1. Learn to handle NLP datasets using PyTorch's Dataset and DataLoader.\n",
        "2. Implement custom processing and loading methods for NLP data.\n",
        "\n",
        "#### Task Instructions:\n",
        "\n",
        "1. **Initialize the Dataset**:\n",
        "   - In the `__init__` method of the `HGDataset` class, complete the code to extract questions and answers from the provided dataset.\n",
        "   - Assign the questions to `self.input_x` and the answers to `self.target`.\n",
        "   - Ensure that each question corresponds to its respective answer by asserting that `input_x` and `target` are of the same length.\n",
        "   - Store the dataset split (e.g., 'train', 'test') in `self.split` for future reference.\n",
        "\n",
        "2. **Implement the `__getitem__` Method**:\n",
        "   - This method should return a single instance of your data. Implement the `__getitem__` method to return a dictionary containing the input question, the target answer, and the split information for a given index `idx`.\n",
        "   - Ensure the method correctly maps an index to the corresponding question and answer in the dataset.\n",
        "\n",
        "3. **Implement the `__len__` Method**:\n",
        "   - This method should return the total number of items in the dataset. Implement the `__len__` method to return the size of `self.input_x`, which corresponds to the total number of questions (and answers).\n",
        "\n",
        "#### Usage:\n",
        "After completing the `HGDataset` class, you can create instances of the dataset for the training and validation splits:\n",
        "\n",
        "```python\n",
        "dataset_name = 'nq_open'  # This is the name of the dataset we are using\n",
        "\n",
        "# Load the raw dataset\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "# Create instances of HGDataset for training and validation\n",
        "train_dataset = HGDataset(dataset['train'][:10000], 'train')\n",
        "test_dataset = HGDataset(dataset['validation'][:], 'test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:40.471474Z",
          "iopub.status.busy": "2024-03-10T12:06:40.471110Z",
          "iopub.status.idle": "2024-03-10T12:06:49.282177Z",
          "shell.execute_reply": "2024-03-10T12:06:49.281177Z",
          "shell.execute_reply.started": "2024-03-10T12:06:40.471438Z"
        },
        "id": "d9fICEm2EBnT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class HGDataset(torch.utils.data.Dataset):\n",
        "    # longest first for batch finder\n",
        "    def __init__(self, dataset, split):\n",
        "        ### Your code here\n",
        "        input_x = dataset['question']\n",
        "        target = [t[0] for t in dataset['answer']]\n",
        "        self.input_x = input_x\n",
        "        self.target = target\n",
        "        self.split = split\n",
        "        assert len(input_x) == len(target) # test if the condition is true\n",
        "    def __getitem__(self, idx):\n",
        "        ### Your code here\n",
        "        input = self.input_x[idx]\n",
        "        target = self.target[idx]\n",
        "        return{\n",
        "            'input': input,\n",
        "            'target': target,\n",
        "            'split': self.split,\n",
        "        }\n",
        "    def __len__(self):\n",
        "        ### Your code here\n",
        "        return len(self.input_x)\n",
        "\n",
        "\n",
        "dataset_name = 'nq_open'\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "# You can adjust the dataset scale with your own preference\n",
        "# The total number for training is 87,9K, validation is 3.61K\n",
        "train_dataset = HGDataset(dataset['train'][:1000], 'train')\n",
        "test_dataset = HGDataset(dataset['validation'][:100], 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8_q7uaNE7hs"
      },
      "source": [
        "# Task4: Implementing Tokenization and Data Collation\n",
        "\n",
        "In this task, you will set up tokenization parameters and implement a custom data collator function for an NLP model. This is essential for preparing text data properly before feeding it into a neural network for training or inference. Follow the instructions below to configure tokenization and complete the missing parts of the `data_collator_customized` function.\n",
        "\n",
        "#### Objectives:\n",
        "1. Configure tokenizer parameters for handling different aspects of text processing.\n",
        "2. Implement a custom logic for batching and preparing NLP data.\n",
        "\n",
        "\n",
        "#### Task Instructions:\n",
        "\n",
        "1. **Implement Custom Data Collator**:\n",
        "   - In the `data_collator_customized` function, complete the code to transform a list of feature dictionaries into a unified batch for model processing.\n",
        "   - Collect batched features by accumulating each key's values from the feature dictionaries.\n",
        "   - Determine the batch's split type (training or inference) based on the 'split' field from the input features.\n",
        "   - Prepare the text for tokenization by concatenating questions and answers, adding special tokens as necessary. Use different formatting based on whether the batch is for training or inference.\n",
        "   - Tokenize the concatenated text using the appropriate tokenizer (Suggestion: `rtokenizer` for processing concatenated text, `tokenizer` for inference scenarios, but you can choose your own preference nevertheless).\n",
        "   - Prepare the final batch dictionary need have `input_ids`, `attention_mask`, and `labels` keys.\n",
        "\n",
        "#### Usage:\n",
        "After completing the setup and implementation, your custom data collator will be used to prepare data batches in trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:49.284239Z",
          "iopub.status.busy": "2024-03-10T12:06:49.283789Z",
          "iopub.status.idle": "2024-03-10T12:06:51.430077Z",
          "shell.execute_reply": "2024-03-10T12:06:51.429206Z",
          "shell.execute_reply.started": "2024-03-10T12:06:49.284199Z"
        },
        "id": "ssS429twEhVu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "MAX_TOKEN_LENGTH = 128\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = 'left'\n",
        "tokenizer.truncation_side = 'left'\n",
        "\n",
        "rtokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "rtokenizer.padding_side = 'right'\n",
        "rtokenizer.truncation_side = 'right'\n",
        "\n",
        "def data_collator_customized(features, return_tensors=\"pt\"):\n",
        "    batch = {}\n",
        "    ### Your code here\n",
        "    batchfied_features = {}\n",
        "    keys = features[0].keys()\n",
        "    for key in keys:\n",
        "        batchfied_features[key]=[f[key] for f in features]\n",
        "    split = batchfied_features['split'][0]\n",
        "    for_inference = (split == 'test')\n",
        "    input_text = batchfied_features['input']\n",
        "    target_text = batchfied_features['target']\n",
        "    bos_token = tokenizer.bos_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    eos_token = tokenizer.eos_token\n",
        "    concated_text = [f'{bos_token}Question: {i}. Answer:{t}{eos_token}' for i, t in zip(input_text, target_text)]\n",
        "    lm_input = rtokenizer(concated_text, add_special_tokens=False, return_tensors='pt', padding = 'max_length', truncation = True, max_length=MAX_TOKEN_LENGTH)\n",
        "    lm_target = lm_input.copy()\n",
        "    lm_target = lm_target['input_ids'][:, :]\n",
        "    batch = {**lm_input, 'labels': lm_target}\n",
        "    if for_inference:\n",
        "        concated_text = [f'{bos_token}Question:{i}. Answer:' for i in input_text]\n",
        "        lm_input = tokenizer(concated_text, add_special_tokens=False, return_tensors='pt',\n",
        "                             padding=True, truncation = True, \n",
        "                             max_length=MAX_TOKEN_LENGTH)\n",
        "        batch = lm_input\n",
        "    ### End of code writing\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mqw_9BrFwOg"
      },
      "source": [
        "# Setting Up the Seq2Seq Trainer\n",
        "\n",
        "The provided code block initializes and configures a `Seq2SeqTrainer` from the Hugging Face's Transformers library. This trainer is designed specifically for sequence-to-sequence models, such as those used in translation, summarization, and other NLP tasks where both the input and output are sequences of tokens.\n",
        "\n",
        "#### Trainer Configuration:\n",
        "- `model`: This is the sequence-to-sequence model that you will be training.\n",
        "- `train_dataset`: The dataset used for training the model.\n",
        "- `args`: A set of training arguments configuring how the model should be trained. These arguments include:\n",
        "    - `per_device_train_batch_size`: The batch size per device during training.\n",
        "    - `per_device_eval_batch_size`: The batch size per device during evaluation.\n",
        "    - `gradient_accumulation_steps`: The number of steps to accumulate gradients before performing a backward/update pass.\n",
        "    - `warmup_steps`: The number of steps used for the warm-up phase.\n",
        "    - `num_train_epochs`: The total number of training epochs.\n",
        "    - `learning_rate`: The initial learning rate for the Adam optimizer.\n",
        "    - `bf16`: Enables training using bfloat16 precision on supported GPUs, which can improve performance and reduce memory usage.\n",
        "    - `logging_steps`: How often to log training information.\n",
        "    - `report_to`: Determines the integration backends where the logs should be reported (here, logging is disabled with 'none').\n",
        "    - `remove_unused_columns`: Indicates whether columns not used by the model forward pass should be removed.\n",
        "    - `output_dir`: The directory where the training outputs should be saved.\n",
        "    - `generation_config`: Specific generation settings for the evaluation phase, such as `max_length` and `num_beams`.\n",
        "    - `predict_with_generate`: Determines whether predictions should be made by generating text during evaluation.\n",
        "\n",
        "- `data_collator`: Custom function to prepare the data batches for training and evaluation.\n",
        "\n",
        "# Training Process:\n",
        "- `trainer.train()`: This method starts the training process based on the configurations provided.\n",
        "\n",
        "The trainer automates many tasks associated with training a sequence-to-sequence model, including data loading, batch creation, model optimization, and logging, allowing you to focus on model architecture and data rather than boilerplate training code.\n",
        "\n",
        "---\n",
        "\n",
        "This setup is typical for training state-of-the-art NLP models and provides a flexible, high-level interface for custom sequence-to-sequence tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:06:51.431681Z",
          "iopub.status.busy": "2024-03-10T12:06:51.431287Z",
          "iopub.status.idle": "2024-03-10T12:14:29.501356Z",
          "shell.execute_reply": "2024-03-10T12:14:29.500411Z",
          "shell.execute_reply.started": "2024-03-10T12:06:51.431646Z"
        },
        "id": "D-KRUG-BFO2y",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "  0%|          | 0/1000 [23:32:55<?, ?it/s]\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 27\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mSeq2SeqTrainer(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator_customized\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2902\u001b[0m, in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhyperparameter_search\u001b[39m(\n\u001b[0;32m   2861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2862\u001b[0m     hp_space: Optional[Callable[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptuna.Trial\u001b[39m\u001b[38;5;124m\"\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2868\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2869\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[BestRun, List[BestRun]]:\n\u001b[0;32m   2870\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;124;03m    Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\u001b[39;00m\n\u001b[0;32m   2872\u001b[0m \u001b[38;5;124;03m    by `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\u001b[39;00m\n\u001b[0;32m   2873\u001b[0m \u001b[38;5;124;03m    the sum of all metrics otherwise.\u001b[39;00m\n\u001b[0;32m   2874\u001b[0m \n\u001b[0;32m   2875\u001b[0m \u001b[38;5;124;03m    <Tip warning={true}>\u001b[39;00m\n\u001b[0;32m   2876\u001b[0m \n\u001b[0;32m   2877\u001b[0m \u001b[38;5;124;03m    To use this method, you need to have provided a `model_init` when initializing your [`Trainer`]: we need to\u001b[39;00m\n\u001b[0;32m   2878\u001b[0m \u001b[38;5;124;03m    reinitialize the model at each new run. This is incompatible with the `optimizers` argument, so you need to\u001b[39;00m\n\u001b[0;32m   2879\u001b[0m \u001b[38;5;124;03m    subclass [`Trainer`] and override the method [`~Trainer.create_optimizer_and_scheduler`] for custom\u001b[39;00m\n\u001b[0;32m   2880\u001b[0m \u001b[38;5;124;03m    optimizer/scheduler.\u001b[39;00m\n\u001b[0;32m   2881\u001b[0m \n\u001b[0;32m   2882\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[0;32m   2883\u001b[0m \n\u001b[0;32m   2884\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   2885\u001b[0m \u001b[38;5;124;03m        hp_space (`Callable[[\"optuna.Trial\"], Dict[str, float]]`, *optional*):\u001b[39;00m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m            A function that defines the hyperparameter search space. Will default to\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m            [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\u001b[39;00m\n\u001b[0;32m   2888\u001b[0m \u001b[38;5;124;03m            [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\u001b[39;00m\n\u001b[0;32m   2889\u001b[0m \u001b[38;5;124;03m        compute_objective (`Callable[[Dict[str, float]], float]`, *optional*):\u001b[39;00m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;124;03m            A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\u001b[39;00m\n\u001b[0;32m   2891\u001b[0m \u001b[38;5;124;03m            method. Will default to [`~trainer_utils.default_compute_objective`].\u001b[39;00m\n\u001b[0;32m   2892\u001b[0m \u001b[38;5;124;03m        n_trials (`int`, *optional*, defaults to 100):\u001b[39;00m\n\u001b[0;32m   2893\u001b[0m \u001b[38;5;124;03m            The number of trial runs to test.\u001b[39;00m\n\u001b[0;32m   2894\u001b[0m \u001b[38;5;124;03m        direction (`str` or `List[str]`, *optional*, defaults to `\"minimize\"`):\u001b[39;00m\n\u001b[0;32m   2895\u001b[0m \u001b[38;5;124;03m            If it's single objective optimization, direction is `str`, can be `\"minimize\"` or `\"maximize\"`, you\u001b[39;00m\n\u001b[0;32m   2896\u001b[0m \u001b[38;5;124;03m            should pick `\"minimize\"` when optimizing the validation loss, `\"maximize\"` when optimizing one or\u001b[39;00m\n\u001b[0;32m   2897\u001b[0m \u001b[38;5;124;03m            several metrics. If it's multi objectives optimization, direction is `List[str]`, can be List of\u001b[39;00m\n\u001b[0;32m   2898\u001b[0m \u001b[38;5;124;03m            `\"minimize\"` and `\"maximize\"`, you should pick `\"minimize\"` when optimizing the validation loss,\u001b[39;00m\n\u001b[0;32m   2899\u001b[0m \u001b[38;5;124;03m            `\"maximize\"` when optimizing one or several metrics.\u001b[39;00m\n\u001b[0;32m   2900\u001b[0m \u001b[38;5;124;03m        backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\u001b[39;00m\n\u001b[0;32m   2901\u001b[0m \u001b[38;5;124;03m            The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\u001b[39;00m\n\u001b[1;32m-> 2902\u001b[0m \u001b[38;5;124;03m            on which one is installed. If all are installed, will default to optuna.\u001b[39;00m\n\u001b[0;32m   2903\u001b[0m \u001b[38;5;124;03m        hp_name (`Callable[[\"optuna.Trial\"], str]]`, *optional*):\u001b[39;00m\n\u001b[0;32m   2904\u001b[0m \u001b[38;5;124;03m            A function that defines the trial/run name. Will default to None.\u001b[39;00m\n\u001b[0;32m   2905\u001b[0m \u001b[38;5;124;03m        kwargs (`Dict[str, Any]`, *optional*):\u001b[39;00m\n\u001b[0;32m   2906\u001b[0m \u001b[38;5;124;03m            Additional keyword arguments passed along to `optuna.create_study` or `ray.tune.run`. For more\u001b[39;00m\n\u001b[0;32m   2907\u001b[0m \u001b[38;5;124;03m            information see:\u001b[39;00m\n\u001b[0;32m   2908\u001b[0m \n\u001b[0;32m   2909\u001b[0m \u001b[38;5;124;03m            - the documentation of\u001b[39;00m\n\u001b[0;32m   2910\u001b[0m \u001b[38;5;124;03m              [optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)\u001b[39;00m\n\u001b[0;32m   2911\u001b[0m \u001b[38;5;124;03m            - the documentation of [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)\u001b[39;00m\n\u001b[0;32m   2912\u001b[0m \u001b[38;5;124;03m            - the documentation of [sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)\u001b[39;00m\n\u001b[0;32m   2913\u001b[0m \n\u001b[0;32m   2914\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   2915\u001b[0m \u001b[38;5;124;03m        [`trainer_utils.BestRun` or `List[trainer_utils.BestRun]`]: All the information about the best run or best\u001b[39;00m\n\u001b[0;32m   2916\u001b[0m \u001b[38;5;124;03m        runs for multi-objective optimization. Experiment summary can be found in `run_summary` attribute for Ray\u001b[39;00m\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;124;03m        backend.\u001b[39;00m\n\u001b[0;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2920\u001b[0m         backend \u001b[38;5;241m=\u001b[39m default_hp_search_backend()\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2925\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2923\u001b[0m backend_obj\u001b[38;5;241m.\u001b[39mensure_available()\n\u001b[0;32m   2924\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_search_backend \u001b[38;5;241m=\u001b[39m backend\n\u001b[1;32m-> 2925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2926\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2927\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use hyperparameter search, you need to pass your model through a model_init function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2928\u001b[0m     )\n\u001b[0;32m   2930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_space \u001b[38;5;241m=\u001b[39m backend_obj\u001b[38;5;241m.\u001b[39mdefault_hp_space \u001b[38;5;28;01mif\u001b[39;00m hp_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m hp_space\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:822\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:810\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\peft_model.py:1129\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1128\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:1128\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1125\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1128\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m   1142\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:884\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    881\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[idx] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m--> 884\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    895\u001b[0m         hidden_states,\n\u001b[0;32m    896\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    900\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    901\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     set_eval_frame(prior)\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_dynamo\\external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:482\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    479\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m         )\n\u001b[1;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[0;32m    485\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    486\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py:261\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[1;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[0;32m    258\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 261\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:535\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    532\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    543\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:263\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# partitioned aross GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m--> 263\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights_reshaped, past_key_value\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\seoeunl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "batch_size = 1\n",
        "trainer = transformers.Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    args=transformers.Seq2SeqTrainingArguments(\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=0,\n",
        "        num_train_epochs=1.0,\n",
        "        learning_rate=0.0001,\n",
        "        bf16=False, # If your GPU supports, make it True\n",
        "        fp16=True, # Since we disable the bf16, we use FP16 instead\n",
        "        logging_steps=1,\n",
        "        report_to=['none'],\n",
        "        remove_unused_columns=False,\n",
        "        output_dir='model_output',\n",
        "        generation_config=transformers.GenerationConfig(\n",
        "            max_length=5,\n",
        "            num_beams=1,\n",
        "        ),\n",
        "        predict_with_generate=True,\n",
        "    ),\n",
        "    data_collator=data_collator_customized\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kryezKcG59i"
      },
      "source": [
        "# Processing Evaluation Results\n",
        "\n",
        "The provided code block is part of an evaluation process, typically after a model has been used to generate predictions on a dataset. This section explains the purpose of each line in the code:\n",
        "\n",
        "1. `logits = eval_result.predictions`:\n",
        "    - This line retrieves the predicted logits from the evaluation results. Logits are the raw output scores from the model before applying any activation function like softmax. In the context of NLP and sequence generation, these logits represent the scores assigned to each token being the next token in the sequence.\n",
        "\n",
        "2. `logits[logits == -100] = tokenizer.eos_token_id`:\n",
        "    - Here, the code is handling a specific case where certain values in the logits are marked with -100. Typically, -100 is used as a masking value in NLP tasks to ignore specific tokens in loss calculations, such as padded tokens or special tokens that should not contribute to the model's learning.\n",
        "    - This line replaces all instances of -100 in the logits with the end-of-sequence (EOS) token ID from the tokenizer. The EOS token is used to signify the end of a text sequence. This replacement ensures that when converting logits back to text, the sequence ends appropriately at the designated end-of-sequence markers instead of continuing with tokens corresponding to -100, which are effectively placeholder or non-token values.\n",
        "\n",
        "3. `text_result = []`:\n",
        "    - This line initializes an empty list, `text_result`, which will be used to store the final generated text sequences after converting the logits (or token IDs) back into human-readable text.\n",
        "\n",
        "---\n",
        "\n",
        "This code is typically part of a larger process where the model's predictions (logits) are converted into text sequences. The logits are first cleaned or adjusted as necessary (e.g., replacing placeholder values with EOS token IDs), and then each sequence of logits is decoded into text, usually involving additional steps not shown here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:14:29.502838Z",
          "iopub.status.busy": "2024-03-10T12:14:29.502550Z",
          "iopub.status.idle": "2024-03-10T12:14:59.593419Z",
          "shell.execute_reply": "2024-03-10T12:14:59.592510Z",
          "shell.execute_reply.started": "2024-03-10T12:14:29.502813Z"
        },
        "id": "876i9Z9KG91F",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "eval_result = trainer.predict(test_dataset, max_new_tokens=5)\n",
        "logits = eval_result.predictions\n",
        "logits[logits == -100] = tokenizer.eos_token_id\n",
        "text_result = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGaLbAUSHDgf"
      },
      "source": [
        "# Task5: Decoding Model Predictions\n",
        "\n",
        "In this task, you will write code to decode the raw text predictions from your NLP model and process them to extract meaningful output. You will work with the logits generated by the model after evaluation and use them to produce human-readable text. Follow the instructions below to complete the missing parts of the code.\n",
        "\n",
        "#### Objectives:\n",
        "1. Decode raw model predictions into text.\n",
        "2. Process the decoded text to extract and clean the answers.\n",
        "\n",
        "#### Task Instructions:\n",
        "\n",
        "1. **Decode the Logits**:\n",
        "   - Use the `tokenizer.batch_decode` method to convert the `logits` obtained from model predictions into raw text sequences. Assign the result to `raw_text_result`.\n",
        "   - This method converts token IDs back into strings, making them human-readable.\n",
        "\n",
        "2. **Extract Context for Evaluation**:\n",
        "   - Retrieve the original questions (context) from `test_dataset` for reference or comparison. Store these in a list called `context` by iterating over the dataset and accessing the 'input' field for each item.\n",
        "\n",
        "3. **Process Decoded Text**:\n",
        "   - Initialize an empty list `text_result` to store the processed answers.\n",
        "   - Iterate over each decoded text sequence in `raw_text_result`:\n",
        "       - Remove any padding tokens from the text using the `replace` method.\n",
        "       - Find the index of the keyword 'Answer:' which separates the question from the answer in the decoded text.\n",
        "       - Extract the answer text by slicing the string from just after 'Answer:' to the end of the text.\n",
        "       - Check if the EOS (end-of-sequence) token exists in the answer and truncate the answer at this token to remove any trailing text.\n",
        "       - Append the cleaned answer to `text_result`.\n",
        "\n",
        "4. **Extract Ground Truth for Comparison**:\n",
        "   - Similar to extracting the context, retrieve the ground truth answers from `test_dataset` by accessing the 'target' field for each item. Store these in a list called `ground_truth`.\n",
        "\n",
        "5. **Output Results**:\n",
        "   - Print the first 10 entries of `text_result` to verify the decoding and processing steps.\n",
        "\n",
        "#### Usage:\n",
        "After completing the above steps, your code will effectively process the raw predictions from your NLP model into a clean, human-readable format, suitable for evaluation against the ground truth answers.\n",
        "\n",
        "```python\n",
        "# Place your completed code here based on the above instructions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:32:43.246181Z",
          "iopub.status.busy": "2024-03-10T12:32:43.245778Z",
          "iopub.status.idle": "2024-03-10T12:32:43.258630Z",
          "shell.execute_reply": "2024-03-10T12:32:43.257537Z",
          "shell.execute_reply.started": "2024-03-10T12:32:43.246150Z"
        },
        "id": "xbZJ7hhtHkkv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your code here\n",
        "\n",
        "### End of Your code\n",
        "ground_truth = [test_dataset.__getitem__(i)['target'] for i in range(test_dataset.__len__())]\n",
        "for question, pred, gt in list(zip(context, text_result, ground_truth))[:10]:\n",
        "    print(f\"\"\"\n",
        "    Question: {question}\n",
        "    Prediction: {pred}\n",
        "    Ground Truth: {gt}\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE4PQcoiIAti"
      },
      "source": [
        "# Calculating Evaluation Metrics\n",
        "\n",
        "The provided code block is involved in evaluating the performance of an NLP model, specifically focusing on text generation tasks such as translation, summarization, or question-answering. It calculates two common metrics used in NLP: BLEU and ROUGE. Here is what each part of the code is responsible for:\n",
        "\n",
        "1. `bleu_score = bleu_scorer.compute(predictions=text_result, references=ground_truth)`:\n",
        "    - This line computes the BLEU (Bilingual Evaluation Understudy) score for the model's predictions against the ground truth answers. BLEU measures the correspondence between the machine-generated text and one or more reference texts. It does this by comparing the presence and frequency of phrases in the generated text to those in the reference text(s), effectively quantifying the quality of the generated text. High BLEU scores indicate better matching with the reference, suggesting better translation or summarization quality.\n",
        "\n",
        "2. `rouge_score = rouge_scorer.compute(predictions=text_result, references=ground_truth)`:\n",
        "    - This line computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score. ROUGE is used primarily to evaluate text summarization quality but can also be applied to other generation tasks. Unlike BLEU, which is precision-oriented, ROUGE focuses on recall — it measures the amount of overlap (in terms of n-grams, word sequences, and word pairs) between the generated text and the reference texts. There are different variations of ROUGE, each focusing on different aspects of the texts' overlap. A higher ROUGE score indicates that more elements of the reference texts were captured in the generated text.\n",
        "\n",
        "3. `print(bleu_score)`, `print(rouge_score)`:\n",
        "    - These lines output the calculated BLEU and ROUGE scores, respectively. These scores give an indication of how well the generated text matches the expected output according to different metrics of textual similarity and quality.\n",
        "\n",
        "---\n",
        "\n",
        "By using these metrics, developers and researchers can quantitatively assess the performance of their NLP models in tasks like translation, summarization, or question answering. It helps in understanding the model's capabilities and areas for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-10T12:31:33.273634Z",
          "iopub.status.busy": "2024-03-10T12:31:33.273222Z",
          "iopub.status.idle": "2024-03-10T12:31:33.529501Z",
          "shell.execute_reply": "2024-03-10T12:31:33.528352Z",
          "shell.execute_reply.started": "2024-03-10T12:31:33.273599Z"
        },
        "id": "yxo11uWjH_So",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bleu_score = bleu_scorer.compute(predictions=text_result, references=ground_truth)\n",
        "rouge_score = rouge_scorer.compute(predictions=text_result, references=ground_truth)\n",
        "print('BLEU1:', bleu_score['precisions'][0]*100)\n",
        "print(f\"\"\"\n",
        "ROUGE-1: {rouge_score['rouge1']*100}\n",
        "ROUGE-2: {rouge_score['rouge2']*100}\n",
        "ROUGE-L: {rouge_score['rougeL']*100}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZJzXLYvHt18"
      },
      "source": [
        "# Task6: Improve task performance & play with the model\n",
        "- From the last implementation, you can easily find the\n",
        "- You can improve the task performance by trying different model\n",
        "- You can play with the model, to give it some questions you like, and check what results can be outputted."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
