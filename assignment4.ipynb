{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers[torch]\n",
    "# !pip install -q datasets\n",
    "# !pip install -q accelerate -U\n",
    "# !pip install -q py7zr\n",
    "# !pip install -q evaluate nltk rouge_score\n",
    "# !pip install peft\n",
    "# !pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Evaluator\n",
    "\n",
    "In this section of the code, we are initializing evaluation metrics for the language model we plan to fine-tune. Specifically, we use the `evaluate` library, which is a part of the Hugging Face ecosystem, designed for evaluating and comparing the performance of models across a wide range of NLP tasks.\n",
    "\n",
    "1. `bleu_scorer = evaluate.load('bleu')`: This line loads the BLEU (Bilingual Evaluation Understudy) scorer from the `evaluate` library. BLEU is a widely used metric for evaluating the quality of text which has been machine-translated from one natural language to another. It works by comparing the machine-generated text to one or more reference texts (typically human-generated) and computes a score indicating how similar they are, based on the presence of the same words and phrases. BLEU is particularly popular in tasks like machine translation but is also used in other contexts like text summarization.\n",
    "\n",
    "2. `rouge_scorer = evaluate.load('rouge')`: This line loads the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scorer. ROUGE is another popular evaluation metric used primarily in summarization tasks. Unlike BLEU, which is precision-oriented, ROUGE focuses on recall, meaning it measures how well the generated summaries cover the content present in the reference summaries. It compares the overlap of n-grams, word sequences, and word pairs between the computer-generated output and the reference texts.\n",
    "\n",
    "These metrics will be used later in the training process to evaluate how well the fine-tuned language model performs on specific NLP tasks, such as translation or summarization. Using these evaluation metrics allows us to quantitatively assess the quality of the generated text and make informed decisions about the model's performance and potential improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu_scorer = evaluate.load('bleu')\n",
    "rouge_scorer = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_name = \"facebook/opt-2.7b\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = pd.read_csv(\"training_corpus.csv\")\n",
    "test_dataset = pd.read_csv(\"assignments/6595203.csv\")\n",
    "\n",
    "# dataset_len = len(dataset)\n",
    "# dataset = dataset[:dataset_len/2]\n",
    "\n",
    "# test_dataset_len = len(test_dataset)\n",
    "# test_dataset = dataset[:test_dataset_len/2]\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    # longest first for batch finder\n",
    "    def __init__(self, dataset, split):\n",
    "        input_x = dataset['context']\n",
    "        target = dataset['target']\n",
    "        self.input_x = input_x\n",
    "        self.target = target\n",
    "        self.split = split\n",
    "        assert len(input_x) == len(target) # test if the condition is true\n",
    "    def __getitem__(self, idx):\n",
    "        ### Your code here\n",
    "        input = self.input_x[idx]\n",
    "        target = self.target[idx]\n",
    "        return{\n",
    "            'input': input,\n",
    "            'target': target,\n",
    "            'split': self.split,\n",
    "        }\n",
    "    def __len__(self):\n",
    "        ### Your code here\n",
    "        return len(self.input_x)\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    # longest first for batch finder\n",
    "    def __init__(self, dataset, split):\n",
    "        input_x = dataset['context']\n",
    "        self.input_x = input_x\n",
    "        self.split = split\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ### Your code here\n",
    "        input = self.input_x[idx]\n",
    "        return{\n",
    "            'input': input,\n",
    "            'split': self.split,\n",
    "        }\n",
    "    def __len__(self):\n",
    "        ### Your code here\n",
    "        return len(self.input_x)\n",
    "    \n",
    "\n",
    "# You can adjust the dataset scale with your own preference\n",
    "# The total number for training is 87,9K, validation is 3.61K\n",
    "# train_dataset = TrainDataset(dataset['context'][:1000], 'train')\n",
    "# test_dataset = TestDataset(dataset['context'][:100], 'test')\n",
    "# train_dataset = TrainDataset(dataset, 'train')\n",
    "# test_dataset = TestDataset(dataset, 'test')\n",
    "train_dataset = TrainDataset(dataset, 'train')\n",
    "test_dataset = TestDataset(test_dataset, 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "MAX_TOKEN_LENGTH = 128\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "rtokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "rtokenizer.padding_side = 'right'\n",
    "rtokenizer.truncation_side = 'right'\n",
    "\n",
    "def data_collator_customized(features, return_tensors=\"pt\"):\n",
    "    batch = {}\n",
    "    ### Your code here\n",
    "    batchfied_features = {}\n",
    "    keys = features[0].keys()\n",
    "    for key in keys:\n",
    "        batchfied_features[key]=[f[key] for f in features]\n",
    "    split = batchfied_features['split'][0]\n",
    "    for_inference = (split == 'test')\n",
    "\n",
    "    input_text = batchfied_features['input']\n",
    "    bos_token = tokenizer.bos_token\n",
    "    eos_token = tokenizer.eos_token\n",
    "\n",
    "    if for_inference:\n",
    "        concated_text = [f'{bos_token}Context: {i}. Prediction: ' for i in input_text]\n",
    "        lm_input = rtokenizer(concated_text, add_special_tokens=False, return_tensors='pt',\n",
    "                             padding=True, truncation = True, \n",
    "                             max_length=MAX_TOKEN_LENGTH)\n",
    "        return lm_input\n",
    "\n",
    "    target_text = batchfied_features['target']\n",
    "    concated_text = [f'{bos_token}Context: {i}. Prediction:{t}{eos_token}' for i, t in zip(input_text, target_text)]\n",
    "\n",
    "    lm_input = rtokenizer(concated_text, add_special_tokens=False, return_tensors='pt', padding = 'max_length', truncation = True, max_length=MAX_TOKEN_LENGTH)\n",
    "    \n",
    "    lm_target = lm_input.copy()\n",
    "    lm_target = lm_target['input_ids'][:, :]\n",
    "    batch = {**lm_input, 'labels': lm_target}\n",
    "    # if for_inference:\n",
    "    #     concated_text = [f'{bos_token}Context:{i}. Prediction:' for i in input_text]\n",
    "    #     lm_input = tokenizer(concated_text, add_special_tokens=False, return_tensors='pt',\n",
    "    #                          padding=True, truncation = True, \n",
    "    #                          max_length=MAX_TOKEN_LENGTH)\n",
    "    # batch = lm_input\n",
    "    ### End of code writing\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=transformers.Seq2SeqTrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=0,\n",
    "        num_train_epochs=1.0,\n",
    "        learning_rate=0.0001,\n",
    "        bf16=False, # If your GPU supports, make it True\n",
    "        fp16=True, # Since we disable the bf16, we use FP16 instead\n",
    "        logging_steps=1,\n",
    "        report_to=['none'],\n",
    "        remove_unused_columns=False,\n",
    "        output_dir='model_output',\n",
    "        generation_config=transformers.GenerationConfig(\n",
    "            max_length=5,\n",
    "            num_beams=1,\n",
    "        ),\n",
    "        predict_with_generate=True,\n",
    "    ),\n",
    "    data_collator=data_collator_customized\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = trainer.predict(test_dataset, max_new_tokens=96)\n",
    "logits = eval_result.predictions\n",
    "logits[logits == -100] = tokenizer.eos_token_id\n",
    "# text_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text = tokenizer.batch_decode(logits)\n",
    "\n",
    "results = []\n",
    "\n",
    "broken_count = 0\n",
    "\n",
    "for tt in predicted_text:\n",
    "    tt = tt.replace(tokenizer.pad_token, '')\n",
    "    tt = tt.replace('<pad>', '')\n",
    "    # print(tt)\n",
    "    keyword = 'Prediction:'\n",
    "    if keyword in tt:\n",
    "        cc_idx = tt.index(keyword)\n",
    "        # print(cc_idx)\n",
    "        tt = tt[cc_idx + len(keyword):]\n",
    "        if tokenizer.eos_token in tt:\n",
    "            tt = tt[:tt.index(tokenizer.eos_token)]\n",
    "\n",
    "        results.append(tt)\n",
    "    else:\n",
    "        # print(tt)\n",
    "        broken_count += 1\n",
    "\n",
    "results\n",
    "# broken_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = 0\n",
    "\n",
    "new_results = []\n",
    "\n",
    "for x in results:\n",
    "    if x == \"\":\n",
    "        empty += 1\n",
    "        print(\"empty\")\n",
    "    else:\n",
    "        if x.startswith(' '):\n",
    "            x = x[1:]\n",
    "        print(x)\n",
    "    \n",
    "    new_results.append(x)\n",
    "\n",
    "empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataset = pd.read_csv(\"assignments/6595203.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the results to the dataset\n",
    "results_dataset['prediction'] = new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataset.to_csv('6595203.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
